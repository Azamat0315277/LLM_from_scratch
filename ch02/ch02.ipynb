{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "992a7187",
   "metadata": {},
   "source": [
    "# 2. Зейін механизмы (attention mechanisms)\n",
    "# 2. Механизм внимания (attention mechanisms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa543b92",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "1. Бұл блокнотта қолданылатын пакеттер:\n",
    "2. Библиотеки, которые используются в этом блокноте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "615062be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "from sympy.polys.polyconfig import query\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f79d8",
   "metadata": {},
   "source": [
    "- Бұл тарауда зейін механизмдеры қарастырылады\n",
    "- В этой главе мы рассмотрим механизмы внимания."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5ec6c7",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/01.webp?123\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec61187",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/02.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d77fd",
   "metadata": {},
   "source": [
    "## 2.1 Ұзын cөз тізбектерін модельдеу мәселесі\n",
    "## 2.1 Проблема моделирования длинных последовательностей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46379835",
   "metadata": {},
   "source": [
    "- Тілдерді сөзбе сөз дұрыс аудару мүмкін емес, себебі олардың грамматикалық құрылымдары өзгеше\n",
    "***\n",
    "- Дословный перевод (слово в слово) нецелесообразен \n",
    "из-за различий в грамматических структурах между исходным языком и языком перевода"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af65e05",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/03.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0acfc8",
   "metadata": {},
   "source": [
    "- Трансформер модельдер пайда болғанға дейін машиналық аударма міндеттері үшін энкодер-декодерлі RNN құрылымы жиі қолданылды.\n",
    "- Бұл құрылымда `энкодер` бастапқы тілдегі токендер тізбегін өңдейді, ол бүкіл кіріс тізбегінің ықшамдалған көрінісін генерациялау үшін жасырын күйде — нейрондық желі аралық қабаттың бір түрін пайдаланады.\n",
    "\n",
    "***\n",
    "\n",
    "- До появления трансформерных моделей в задачах машинного перевода широко использовались RNN-сети с архитектурой «энкодер-декодер».\n",
    "\n",
    "- В этой схеме `энкодер` обрабатывает последовательность токенов исходного языка, используя скрытое состояние — своего рода промежуточный слой нейронной сети — для создания сжатого представления всей входной последовательности:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54749d43",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/04.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c25c01",
   "metadata": {},
   "source": [
    "## 2.2 Зейін механизмдері арқылы деректердегі тәуелділіктерді (заңдылықтарды) анықтау\n",
    "## 2.2 Улавливание зависимостей (закономерностей) в данных с помощью механизмов внимания"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839039e6",
   "metadata": {},
   "source": [
    "- Зейін механизмі арқылы  декодер барлық кіріс токендеріне таңдамалы түрде қол жеткізе алады. Бұл нақты бір шығыс токенін генерациялау кезінде кейбір кіріс токендерінің басқаларға қарағанда маңыздырақ екенін білдіреді.\n",
    "***\n",
    "- С помощью механизма внимания декодер, генерирующий текст, способен избирательно обращаться ко всем входным токенам. Это означает, что при генерации определённого выходного токена некоторые входные токены имеют большее значение, чем другие."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18da17cc",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/05.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44911456",
   "metadata": {},
   "source": [
    "- Трансформерлердегі өзіндік зейін (self-attention) — бұл тізбектегі әрбір \n",
    "позицияға сол тізбектің барлық басқа позицияларымен өзара әрекеттесуге \n",
    "және олардың маңыздылығын анықтауға мүмкіндік беру арқылы кіріс \n",
    "репрезентацияларын жақсартуға арналған техника.\n",
    "***\n",
    "- Само-внимание (self-attention) в трансформерах — это техника, предназначенная для улучшения входных представлений, позволяющая каждой позиции в последовательности взаимодействовать со всеми другими позициями в той же последовательности и определять их релевантность."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3b18aa",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/06.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bd7ebf",
   "metadata": {},
   "source": [
    "## 2.3 Өзіндік зейін (self-attention) арқылы кіріс деректердің әртүрлі бөліктеріне зейін аудару\n",
    "## 2.3 Фокусировка на различных частях входных данных с помощью само-внимания (self-attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e51e96",
   "metadata": {},
   "source": [
    "### 2.3.1 Үйретілетін салмақтарсыз қарапайым өзіндік зейін механизмі\n",
    "### 2.3.1 Простой механизм само-внимания (self-attention) без обучаемых весов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b815fa9",
   "metadata": {},
   "source": [
    "- Бұл бөлімде ешқандай үйретілетін салмақтары жоқ өзіндік зейіннің (self-attention) өте қарапайым нұсқасы түсіндіріледі.\n",
    "- Бұл тек көрнекілік үшін жасалған және трансформерлерде қолданылатын зейін механизмі ЕМЕС.\n",
    "- Келесі 3.3.2-бөлімде осы қарапайым зейін механизмі нағыз өзіндік зейін механизмін іске асыру үшін кеңейтілетін болады.\n",
    "- Бізге $x^{(1)}$-ден $x^{(T)}$ -ға дейінгі кіріс тізбегі берілді деп есептейік.\n",
    "    - Кіріс деректер — 1-тарауда сипатталғандай, токен эмбеддингтеріне түрлендірілген мәтін (мысалы, \"Your journey starts with one step\" сияқты сөйлем).\n",
    "    - Мысалы, $x^{(1)} - \"Your\" сөзін білдіретін d-өлшемді вектор, және т.б.\n",
    "- **Мақсат**: $x^{(1)}$-ден $x^{(T)}$-ға дейінгі әрбір кіріс \n",
    "тізбегінің элементі $x^{(i)}$ үшін контекстік векторларды $z^{(i)}$ есептеу ($z$ мен $x$ кеңістік өлшемдері бірдей).\n",
    "    - $z^{(i)}$ контекстік векторы — бұл $x^{(1)}$ -ден $x^{(T)}$ -ға дейінгі кіріс     деректерінің салмақталған қосындысы.\n",
    "    - Контекстік вектор белгілі бір кіріс элементі үшін \"контекстке\" арнайы болады.\n",
    "    - Кез келген кіріс токенінің орнына $x^{(i)}$ белгісін қолданудың орнына, екінші кіріс элементін, яғни $x^{(2)}$-ні қарастырайық.\n",
    "    - Нақты мысалды жалғастыру үшін, $z^{(i)}$ белгісінің орнына екінші шығыс контекстік векторын, яғни $z^{(2)}$ -ні қарастырамыз.\n",
    "    - Екінші контекстік вектор, z$z^{(2)}$  екінші кіріс элементіне, $x^{(2)}$ -ге қатысты салмақталған $x^{(1)}$-ден $x^{(T)}$-ға дейінгі барлық кіріс элементтерінің салмақталған қосындысы.\n",
    "    - Зейін салмақтары — бұл $z^{(2)}$-ні есептеу кезінде әрбір кіріс элементінің салмақталған қосындыға қаншалықты үлес қосатынын анықтайтын салмақтар.\n",
    "    - Қысқаша айтқанда, $z^{(2)}$-ні $x^{(2)}$-нің өзгертілген нұсқасы деп ойлаңыз, ол сонымен қатар қарастырылып отырған тапсырма үшін маңызды болып табылатын барлық басқа кіріс элементтері туралы ақпаратты қамтиды."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d203382",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "- В этом разделе объясняется очень упрощённый вариант само-внимания (self-attention), который не содержит обучаемых весов.\n",
    "- Это сделано исключительно в иллюстративных целях и это НЕ тот механизм внимания, который используется в трансформерах.\n",
    "- В следующем разделе, 3.3.2, этот простой механизм внимания будет расширен для реализации настоящего механизма self-attention.\n",
    "\n",
    "- Предположим, нам дана входная последовательность от $x^{(1)}$ до $x^{(T)}$.\n",
    "- Входные данные — это текст (например, предложение \"Your journey starts with one step\"), который уже был преобразован в токенные эмбеддинги, как описано в главе 2.\n",
    "- Например, $x^{(1)}$ — это d-мерный вектор, представляющий слово \"Your\", и так далее.\n",
    "\n",
    "- **Цель**: вычислить контекстные векторы $z^{(i)}$ для каждого элемента входной последовательности $x^{(i)}$ от $x^{(1)}$до $x^{(T)}$ (где $z$ и $x$ имеют одинаковую размерность).\n",
    "    - Контекстный вектор $z^{(i)}$ это взвешенная сумма входных векторов от $x^{(1)}$до $x^{(T)}$.\n",
    "    - Контекстный вектор является специфичным для \"контекста\" определённого входного элемента.\n",
    "    - Вместо того чтобы использовать $x^{(i)}$ как обозначение для произвольного входного токена, давайте рассмотрим второй входной элемент, $x^{(2)}$.\n",
    "    - И чтобы продолжить на конкретном примере, вместо $z^{(i)}$ мы рассмотрим второй выходной контекстный вектор, $z^{(2)}$.\n",
    "    - Второй контекстный вектор, $z^{(2)}$  это взвешенная сумма всех входных векторов от $x^{(1)}$до $x^{(T)}$, взвешенная относительно второго входного элемента, $x^{(2)}$.\n",
    "    - Веса внимания — это коэффициенты, которые определяют, какой вклад вносит каждый из входных элементов во взвешенную сумму при вычислении $z^{(2)}$.\n",
    "    - Короче говоря, думайте о $z^{(2)}$ как о модифицированной версии $x^{(2)}$, которая также включает информацию обо всех других входных элементах, релевантных для текущей задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c94279",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/07.webp\" width=\"1000px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f7e5f0",
   "metadata": {},
   "source": [
    "- Шартты түрде, нормализациядан өтпеген зейін салмақтары `«зейін баллдары` (attention scores) деп аталады,    \n",
    "ал қосындысы 1-ге тең болатын нормализациядан  өткен зейін бағалары `«зейін салмақтары»` (attention weights) деп аталады.\n",
    "\n",
    "***\n",
    "\n",
    "- Условно, ненормализованные веса внимания называют `«оценками внимания»` (attention scores),    \n",
    "в то время как нормализованные оценки, сумма которых равна 1, называют `«весами внимания»` (attention weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f230dd",
   "metadata": {},
   "source": [
    "- **1-қадам**: нормаланбаған $\\omega$  зейін баллдарын есептеу\n",
    "- Айталық, біз екінші кіріс токенін сұраныс ретінде қолданамыз, яғни, $q^{(2)} = x^{(2)}$, бұл жағдайда біз нормаланбаған зейін баллдарын скалярлық көбейтінділер арқылы есептейміз:\n",
    "    - $\\omega_{21} = x^{(1)} q^{(2)\\top}$\n",
    "    - $\\omega_{22} = x^{(2)} q^{(2)\\top}$\n",
    "    - $\\omega_{23} = x^{(3)} q^{(2)\\top}$\n",
    "    - ...\n",
    "    - $\\omega_{2T} = x^{(T)} q^{(2)\\top}$\n",
    "- Жоғарыда, $\\omega$ — нормаланбаған зейін баллдарын (attention scores) белгілеу үшін қолданылатын «омега» грек әрпі.\n",
    "    - $\\omega_{21}$ жазуындағы «21» төменгі индексі кіріс тізбегінің 2-ші элементі 1-ші элементіне қатысты сұраныс ретінде қолданылғанын білдіреді.\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "- **Шаг 1**: Вычислить ненормализованные оценки внимания $\\omega$\n",
    "- Предположим, мы используем второй входной токен в качестве запроса, то есть q^{(2)} = x^{(2)}$. \n",
    "- В этом случае мы вычисляем ненормализованные оценки внимания с помощью скалярных произведений:\n",
    "    - $\\omega_{21} = x^{(1)} q^{(2)\\top}$\n",
    "    - $\\omega_{22} = x^{(2)} q^{(2)\\top}$\n",
    "    - $\\omega_{23} = x^{(3)} q^{(2)\\top}$\n",
    "    - ...\n",
    "    - $\\omega_{2T} = x^{(T)} q^{(2)\\top}$\n",
    "\n",
    "- Выше $\\omega$ — это греческая буква «омега», которая используется для обозначения ненормализованных оценок внимания (attention scores).\n",
    "    - Нижний индекс «21» в записи $\\omega_{21}$ означает, что второй элемент входной последовательности использовался в качестве запроса к первому элементу.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81723c",
   "metadata": {},
   "source": [
    "- 3 өлшемді векторлар түрінде ұсынылған келесі кіріс сөйлем берілген деп алайық (мұнда көрнекілік үшін  өте шағын ендіру өлшемін қолданамыз):\n",
    "- Предположим, у нас есть следующее входное предложение, уже представленное в виде 3-мерных векторов (для наглядности здесь используется очень малая размерность вложений):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5847955d",
   "metadata": {},
   "source": [
    "- Жоғарыда көрсетілген тензордың әр қатары сөзді, ал әр баған ендіру (embedding) өлшемін білдіреді\n",
    "- Бұл бөлімнің негізгі мақсаты — $x^{(2)}$ арқылы $z^{(2)}$ контекст векторының қалай есептелетінін көрсету.\n",
    "- Төмендегі суретте — скалярлық көбейту операциясы арқылы $x^{(2)}$ мен барлық басқа кіріс элементтері арасындағы ω зейін бағаларын есептеу көрсетілген.\n",
    "\n",
    "***\n",
    "\n",
    "- В случае показанного выше тензора каждая строка представляет слово, \n",
    "а каждый столбец — размерность вложения\n",
    "- Основная цель этого раздела — продемонстрировать, как вычисляется контекстный вектор $z^{(2)}$ с использованием  $x^{(2)}$.\n",
    "- Ниже на рисунке изображён  процесса, который заключается в вычислении оценок внимания ω между $x^{(2)}$\n",
    "и всеми другими входными элементами с помощью операции скалярного произведения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aefbdb1",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/08.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37db2fcadce309ae",
   "metadata": {},
   "source": [
    "- Біз $z^{(2)}$ контекст векторын есептеу үшін мысал ретінде $x^{(2)}$ кіріс тізбегінің 2-элементін қолданамыз; осы бөлімнің соңында біз мұны барлық контекст векторларын есептеу үшін жалпылаймыз.\n",
    "- Бірінші қадам — сұраныс (query) $x^{(2)}$ және барлық басқа кіріс токендерінің арасындағы  скалярдық көбейтіндіні есептеу арқылы нормализацияланбаған зейін (attention) баллдарын есептеу:\n",
    "\n",
    "***\n",
    "\n",
    "- Мы используем элемент 2 входной последовательности, $x^{(2)}$, в качестве примера для вычисления контекстного вектора $z^{(2)}$; позже в этом разделе мы обобщим это для вычисления всех контекстных векторов.\n",
    "- Первый шаг — вычислить ненормализованные оценки внимания (unnormalized attention scores), вычислив скалярное произведение между запросом $x^{(2)}$ и всеми остальными входными токенами:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "642e8b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8823, 0.9150, 0.3829],\n",
       "        [0.9593, 0.3904, 0.6009],\n",
       "        [0.2566, 0.7936, 0.9408],\n",
       "        [0.1332, 0.9346, 0.5936],\n",
       "        [0.8694, 0.5677, 0.7411],\n",
       "        [0.4294, 0.8854, 0.5739]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "inputs = torch.rand(6, 3)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9216eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.8823, 0.9150, 0.3829], # Your     (x^1)\n",
    "   [0.9593, 0.3904, 0.6009], # journey  (x^2)\n",
    "   [0.2566, 0.7936, 0.9408], # starts   (x^3)\n",
    "   [0.1332, 0.9346, 0.5936], # with     (x^4)\n",
    "   [0.8694, 0.5677, 0.7411], # one      (x^5)\n",
    "   [0.4294, 0.8854, 0.5739]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11599857",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "**Скалярлық көбейту**  \n",
    "**Скалярное произведение**\n",
    "\n",
    "<pre>   \n",
    " [0.8823, 0.9150, 0.3829]\n",
    "   X      X     X  \n",
    " [0.9593, 0.3904, 0.6009]\n",
    "\n",
    "   ||    ||     ||\n",
    "\n",
    " 0.8464 + 0.3572 + 0.2301 = 1.4337\n",
    "</pre>\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b60ca7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "journey: tensor([0.9593, 0.3904, 0.6009])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # 2nd input token is the query\n",
    "print(f\"journey: {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "97a3fa01fff93479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8571066f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_1: tensor([0.8823, 0.9150, 0.3829]) * query_2: tensor([0.9593, 0.3904, 0.6009]) -> attn_scores_2: 1.4336910247802734\n",
      "tensor([1.4337, 1.4337, 1.1213, 0.8493, 1.5010, 1.1024])\n"
     ]
    }
   ],
   "source": [
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
    "    if i < 1:\n",
    "        print(f\"x_{i+1}: {x_i} * query_2: {query} -> attn_scores_2: {attn_scores_2[i]}\")\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05a7ffd",
   "metadata": {},
   "source": [
    " - Қосымша ескерту: `скалярлық көбейту` - екі векторды элементтер бойынша көбейтуді және алынған көбейтінділерді қосуды білдіреді:\n",
    " - Примечание: `скалярное произведение` по сути представляет собой сокращённый способ записи умножения двух векторов поэлементно с последующим суммированием полученных произведений:    \n",
    "\n",
    " <br>\n",
    "\n",
    "`(0.8823 * 0.9593) + (0.9150 * 0.3904) + (0.3829 * 0.6009) = 1.4337`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "730b0692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4337)\n",
      "tensor(1.4337)\n"
     ]
    }
   ],
   "source": [
    "res = 0.\n",
    "\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx] * query[idx]\n",
    "\n",
    "print(res)\n",
    "print(torch.dot(inputs[0], query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794538a2",
   "metadata": {},
   "source": [
    "- **2-қадам:** нормаланбаған зейін баллдарын (\"омегалар\", ω) 1-ге тең болатындай етіп нормализациялау\n",
    "- Төменде нормаланбаған зейін баллдарын қосындысы 1-ге тең болатындай нормалаудың қарапайым жолы \n",
    "(бұл – түсіндіру үшін пайдалы және модель оқыту тұрақтылығы үшін маңызды шарт):\n",
    "\n",
    "***\n",
    "\n",
    "- Шаг 2: нормализовать ненормализованные оценки внимания («омеги», ω) так, чтобы их сумма равнялась 1\n",
    "- Ниже приведен простой способ нормализовать ненормализованные оценки внимания так, чтобы их сумма равнялась 1 (это полезно для интерпретации и важно для стабильности обучения):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9307b698",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/09.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f80e7be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4337, 1.4337, 1.1213, 0.8493, 1.5010, 1.1024])\n",
      "tensor(7.4415)\n"
     ]
    }
   ],
   "source": [
    "print(attn_scores_2)\n",
    "print(attn_scores_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e8a069cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1927, 0.1927, 0.1507, 0.1141, 0.2017, 0.1481])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8f0083b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element: 1.4337 / Sum: 7.4415 ---> 0.1927\n"
     ]
    }
   ],
   "source": [
    "print(f\"First element: {attn_scores_2[0]:.4f} / Sum: {attn_scores_2.sum():.4f} ---> {attn_scores_2[0] / attn_scores_2.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c389383d",
   "metadata": {},
   "source": [
    "- Алайда, тәжірибеде `softmax` функциясын нормалау үшін қолдану жиі кездеседі. Себебі, ол экстремалды мәндерді жақсы өңдейді және оқыту кезінде неғұрлым қолайлы градиенттік қасиеттерге ие.\n",
    "- Төменде масштабтауға арналған `softmax` функциясының қарапайым іске асырылуы, ол сонымен қатар вектор элементтерін қосындысы 1-ге тең болатындай нормалайды:\n",
    "\n",
    "***\n",
    "\n",
    "- Однако на практике для нормализации обычно используется и рекомендуется функция `softmax`, которая лучше справляется с экстремальными значениями и обладает более желательными градиентными свойствами во время обучения.\n",
    "-  Ниже простая  реализация функции `softmax` для масштабирования, которая также нормализует элементы вектора так, что их сумма равняется 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ed0d6f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1970, 0.1970, 0.1441, 0.1098, 0.2107, 0.1414])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e86c2e2",
   "metadata": {},
   "source": [
    "- Жоғарыдағы қарапайым функция үлкен немесе кіші кіріс мәндері үшін `толып кету` `(overflow)` және `толықсымау` `(underflow)` мәселелеріне байланысты `cандық тұрақсыздық (numerical instability)` мәселелеріне ұшырауы мүмкін.\n",
    "- Сондықтан, тәжірибеде өнімділік үшін жоғары оңтайландырылған `PyTorch`-тың softmax іске асырылуын қолдану ұсынылады:\n",
    "\n",
    "***\n",
    "\n",
    "- Вышеприведённая простая  реализация может страдать от проблем `численной нестабильности` для больших или малых входных значений из-за `переполнения (overflow)` и `андерфлоу (underflow).`\n",
    "- Следовательно, на практике рекомендуется использовать вместо этого реализацию softmax из `PyTorch`, которая была высоко оптимизирована для производительности:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5036cdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1970, 0.1970, 0.1441, 0.1098, 0.2107, 0.1414])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8431ced5",
   "metadata": {},
   "source": [
    "- **3-қадам:** кірістің ендірілген токендерін $x^{(i)}$ зейін салмақтарына (attention weights) көбейту және алынған векторларды қосу арқылы  $z^{(2)}$ контекст векторын  есептеу:\n",
    "***\n",
    "- **Шаг 3:** вычислить контекстный вектор $z^{(2)}$, умножив встроенные входные токены $x^{(i)}$ на веса внимания (attention weights) и просуммировав полученные векторы:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589fa832",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/10.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "baed893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.8823, 0.9150, 0.3829], # Your     (x^1)\n",
    "   [0.9593, 0.3904, 0.6009], # journey  (x^2)\n",
    "   [0.2566, 0.7936, 0.9408], # starts   (x^3)\n",
    "   [0.1332, 0.9346, 0.5936], # with     (x^4)\n",
    "   [0.8694, 0.5677, 0.7411], # one      (x^5)\n",
    "   [0.4294, 0.8854, 0.5739]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9cace937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: (a_2) tensor([0.1970, 0.1970, 0.1441, 0.1098, 0.2107, 0.1414])\n"
     ]
    }
   ],
   "source": [
    "print(\"Attention weights: (a_2)\", attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "737a65e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = inputs[1] # 2nd input token is the query\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "09cb5d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8823, 0.9150, 0.3829],\n",
       "        [0.9593, 0.3904, 0.6009],\n",
       "        [0.2566, 0.7936, 0.9408],\n",
       "        [0.1332, 0.9346, 0.5936],\n",
       "        [0.8694, 0.5677, 0.7411],\n",
       "        [0.4294, 0.8854, 0.5739]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "269567a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1970, 0.1970, 0.1441, 0.1098, 0.2107, 0.1414])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c5d1325e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1970 * tensor([0.8823, 0.9150, 0.3829])\n",
      "0.1970 * tensor([0.9593, 0.3904, 0.6009])\n",
      "0.1441 * tensor([0.2566, 0.7936, 0.9408])\n",
      "0.1098 * tensor([0.1332, 0.9346, 0.5936])\n",
      "0.2107 * tensor([0.8694, 0.5677, 0.7411])\n",
      "0.1414 * tensor([0.4294, 0.8854, 0.5739])\n",
      "context: tensor([1.9748, 2.1569, 1.8956])\n"
     ]
    }
   ],
   "source": [
    "for i,x_i in enumerate(inputs):\n",
    "    print(f\"{attn_weights_2[i]:.4f} * {x_i}\")\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "\n",
    "print(f\"context: {context_vec_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "28fea4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: (x_2) tensor([0.9593, 0.3904, 0.6009])  ---> Context vector: (z_2) tensor([1.9748, 2.1569, 1.8956])\n"
     ]
    }
   ],
   "source": [
    "print(\"Query: (x_2)\", query, \" ---> Context vector: (z_2)\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac6ec34",
   "metadata": {},
   "source": [
    "### 2.3.2 Барлық кіріс токендері үшін зейін салмақтарын есептеу\n",
    "### 2.3.2 Вычисление весов внимания для всех входных токенов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5be769d",
   "metadata": {},
   "source": [
    "- Жоғарыда біз 2-ші кіріс  токены үшін зейін салмақтарын және контекст векторын есептедік \n",
    "(төмендегі суретте ерекшеленген қатарда көрсетілгендей)\n",
    "- Енді біз барлық зейін салмақтары мен контекст векторларын есептеу үшін бұл есептейміз\n",
    "\n",
    "***\n",
    "\n",
    "- Выше мы вычислили веса внимания и контекстный вектор для второго входа    \n",
    "(как показано в выделенной строке на рисунке ниже)\n",
    "- Далее мы обобщаем это вычисление, чтобы рассчитать все веса внимания и контекстные векторы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb126738",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/11.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d06c24",
   "metadata": {},
   "source": [
    "- Зейін (self-attention) механизмінде процесс алдымен **зейін баллдарын** есептеуден басталады. Бұл баллдар кейіннен қосындысы 1-ге тең болатын **зейін салмақтарын** алу үшін нормализацияланады.\n",
    "- Бұл зейін салмақтары кірістердің **салмақталған қосу** арқылы **контекст векторларын** жасау үшін қолданылады.\n",
    "\n",
    "***\n",
    "\n",
    "- В механизме **self-attention** процесс начинается с вычисления **оценок внимания**, которые затем нормализуются для получения **весов внимания**, в сумме дающих 1.\n",
    "- Эти веса внимания затем используются для создания **контекстных векторов** путём **взвешенного суммирования** входных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d20fe6",
   "metadata": {},
   "source": [
    "- Зейін баллдарын есептеу\n",
    "- Вычисление оценок внимания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1de5c839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.7623, 1.4337, 1.3128, 1.2000, 1.5703, 1.4087],\n",
      "        [1.4337, 1.4337, 1.1213, 0.8493, 1.5010, 1.1024],\n",
      "        [1.3128, 1.1213, 1.5807, 1.3343, 1.3708, 1.3528],\n",
      "        [1.2000, 0.8493, 1.3343, 1.2436, 1.0863, 1.2254],\n",
      "        [1.5703, 1.5010, 1.3708, 1.0863, 1.6274, 1.3013],\n",
      "        [1.4087, 1.1024, 1.3528, 1.2254, 1.3013, 1.2977]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785db08",
   "metadata": {},
   "source": [
    "- Жоғарыдағы есептеу нәтижесін матрицаларды көбейту арқылы тиімдірек қол жеткізуге болады:\n",
    "\n",
    "- Того же результата, что и выше, можно достичь более эффективно с помощью матричного умножения:\n",
    "\n",
    "Link: http://matrixmultiplication.xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa6ebc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.7623, 1.4337, 1.3128, 1.2000, 1.5703, 1.4087],\n",
      "        [1.4337, 1.4337, 1.1213, 0.8493, 1.5010, 1.1024],\n",
      "        [1.3128, 1.1213, 1.5807, 1.3343, 1.3708, 1.3528],\n",
      "        [1.2000, 0.8493, 1.3343, 1.2436, 1.0863, 1.2254],\n",
      "        [1.5703, 1.5010, 1.3708, 1.0863, 1.6274, 1.3013],\n",
      "        [1.4087, 1.1024, 1.3528, 1.2254, 1.3013, 1.2977]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfead80",
   "metadata": {},
   "source": [
    "- Зейін баллдарын нормализациялау\n",
    "- Нормализация оценок внимания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "712ba08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2245, 0.1616, 0.1432, 0.1279, 0.1852, 0.1576],\n",
      "        [0.1970, 0.1970, 0.1441, 0.1098, 0.2107, 0.1414],\n",
      "        [0.1599, 0.1320, 0.2090, 0.1633, 0.1694, 0.1664],\n",
      "        [0.1721, 0.1212, 0.1968, 0.1798, 0.1536, 0.1765],\n",
      "        [0.1926, 0.1797, 0.1578, 0.1187, 0.2039, 0.1472],\n",
      "        [0.1884, 0.1387, 0.1782, 0.1569, 0.1692, 0.1686]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35fa00e",
   "metadata": {},
   "source": [
    "- Зейін баллдарының қосындысы 1ге тең болуын тексеру\n",
    "- Проверка суммы внимания равна ли 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b5cc7323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum([0.1970, 0.1970, 0.1441, 0.1098, 0.2107, 0.1414])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d650d4ab",
   "metadata": {},
   "source": [
    "- Зейін контекст векторларын есептеу\n",
    "-  Вычисление контекстных векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5baafa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6356, 0.7464, 0.6214],\n",
      "        [0.6583, 0.7190, 0.6319],\n",
      "        [0.5618, 0.7598, 0.6552],\n",
      "        [0.5519, 0.7725, 0.6457],\n",
      "        [0.6392, 0.7287, 0.6363],\n",
      "        [0.5855, 0.7599, 0.6384]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dfd45f",
   "metadata": {},
   "source": [
    "## 2.4 Зейін механизмін (attention mechanisms) оқытылатын салмақтармен толықтыру\n",
    "## 2.4 Реализация механизма внимания (self-attention) с обучаемыми весами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e44f586",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/13.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec0fd5",
   "metadata": {},
   "source": [
    "### 2.4.1 Зейін механизмын (attention weights) сатылап жаттықтыру\n",
    "### 2.4.1 Пошаговое вычисление весов внимания (attention weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7560664f",
   "metadata": {},
   "source": [
    "* Бұл бөлімде біз бастапқы трансформер архитектурасында, GPT модельдерінде және басқа да көптеген танымал Үлкен тілдік модельдерде (ҮТМ) қолданылатын **өзіндік зейін (self-attention) механизмін** іске асырамыз.\n",
    "* Бұл өзіндік зейін механизмі сонымен қатар **«масштабталған скаляр көбейтінді зейіні»** (scaled dot-product attention) деп те аталады.\n",
    "* Жалпы идея бұрынғыға ұқсас:\n",
    "    * Біз белгілі бір кіріс элементіне тән кіріс векторлары бойынша **салмақталған қосындылар ретінде контекст векторларын** есептеу керек.\n",
    "    * Ол үшін бізге **зейін салмақтары** қажет.\n",
    "* Бұрын таныстырылған  зейін механизмынан аздаған айырмашылықтар бар:\n",
    "    * Ең айтарлықтай айырмашылық — модельді **жаттықтыру** кезінде жаңартылатын **салмақ матрицаларының** енгізілуі.\n",
    "    * Бұл жаттықтырылатын салмақ матрицалары модельдің (дәлірек айтсақ, модель ішіндегі зейін модулінің) «жақсы» контекст векторларын құрып, үйрену үшін өте маңызды.\n",
    "\n",
    "***\n",
    "\n",
    "* В этом разделе мы реализуем **механизм self-attention (самовнимания)**, который используется в оригинальной архитектуре трансформера, моделях GPT и большинстве других популярных больших языковых моделей (LLM).\n",
    "* Этот механизм self-attention также называют **«масштабированным вниманием на основе скалярного произведения»** (scaled dot-product attention).\n",
    "* Общая идея аналогична предыдущей:\n",
    "    * Мы хотим вычислить **векторы контекста как взвешенные суммы** входных векторов для определённого входного элемента.\n",
    "    * Для этого нам нужны **веса внимания**.\n",
    "* Как вы увидите, существуют лишь незначительные отличия по сравнению с базовым механизмом внимания, представленным ранее:\n",
    "    * Наиболее заметное отличие — это введение **весовых матриц**, которые обновляются во время **обучения** модели.\n",
    "    * Эти обучаемые весовые матрицы крайне важны для того, чтобы модель (в частности, модуль внимания внутри модели) могла научиться создавать «хорошие» векторы контекста."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411c39d8",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/14.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6d479f",
   "metadata": {},
   "source": [
    "* Өзіндік зейін механизмін кезең-кезеңімен іске асыру үшін, біз алдымен үш жаттықтырылатын салмақ матрицасын: $W_q$, $W_k$, және $W_v$ таныстырудан бастаймыз.\n",
    "* Бұл үш матрица ендірілген кіріс токендерін, $x^{(i)}$, матрицалық көбейту арқылы сұраныс (query), кілт (key) және мән (value) векторларына проекциялау үшін қолданылады:\n",
    "\n",
    " - Query vector: $q^{(i)} = x^{(i)}\\,W_q $\n",
    " - Key vector: $k^{(i)} = x^{(i)}\\,W_k $\n",
    " - Value vector: $v^{(i)} = x^{(i)}\\,W_v $\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "* Реализуя механизм self-attention пошагово,   \n",
    "мы начнём с введения трёх обучаемых весовых матриц: $W_q$, $W_k$, және $W_v$.\n",
    "\n",
    "* Эти три матрицы используются для проецирования векторов-вложений (embedded) входных токенов, $x^{(i)}$, в векторы запроса (query), ключа (key) и значения (value) с помощью матричного умножения:\n",
    "\n",
    " - Query vector: $q^{(i)} = x^{(i)}\\,W_q $\n",
    " - Key vector: $k^{(i)} = x^{(i)}\\,W_k $\n",
    " - Value vector: $v^{(i)} = x^{(i)}\\,W_v $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbef63a",
   "metadata": {},
   "source": [
    "* Кіріс $x$ пен сұраныс векторының $q$ ендіру (embedding) өлшемдері модельдің дизайны мен нақты іске асырылуына байланысты бірдей немесе әртүрлі болуы мүмкін.\n",
    "* GPT модельдерінде кіріс және шығыс өлшемдері әдетте бірдей болады, бірақ көрнекілік үшін, есептеуді жақсырақ бақылау мақсатында, біз бұл жерде әртүрлі кіріс және шығыс өлшемдерін таңдаймыз:\n",
    "\n",
    "***\n",
    "\n",
    "* Размерности вложений (embedding dimensions) для входного вектора $x$ и вектора запроса $q$ могут быть одинаковыми или разными, в зависимости от архитектуры модели и конкретной реализации.\n",
    "* В моделях GPT входные и выходные размерности обычно совпадают, но в иллюстративных целях, чтобы было легче следить за вычислениями, мы выберем здесь разные входные и выходные размерности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "083ce036",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] # second input element\n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2120ff72",
   "metadata": {},
   "source": [
    "* Төменде біз үш салмақ матрицасына бастапқы мән береміз; ескеріңіз, көрнекілік мақсатында шығыс деректердегі артық ақпаратты азайту үшін біз `requires_grad=False` параметрін орнаттық. Алайда, егер біз бұл салмақ матрицаларын модельді жаттықтыру үшін қолданатын болсақ, модельді жаттықтыру кезінде осы матрицаларды жаңарту үшін `requires_grad=True` деп белгілер едік.\n",
    "\n",
    "***\n",
    "\n",
    "* Ниже мы инициализируем три весовые матрицы; обратите внимание, что мы устанавливаем `requires_grad=False` в иллюстративных целях, чтобы сделать вывод более наглядным. Однако, если бы мы использовали эти матрицы для обучения модели, мы бы установили `requires_grad=True`, чтобы обновлять их в процессе обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa6f704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# d_in = 3\n",
    "# d_out = 2\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e8bfdf",
   "metadata": {},
   "source": [
    "* Әрі қарай сұраныс, кілт және мән векторларын есептейміз:\n",
    "* Далее вычислим векторы запроса, ключа и значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "16fb3a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4268, 1.2850])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query # _2 because it's with respect to the 2nd input element\n",
    "key_2 = x_2 @ W_key \n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a517cc8e",
   "metadata": {},
   "source": [
    "* Төменде көріп отырғанымыздай, біз 6 кіріс токенін 3D кеңістіктен 2D ендіру кеңістігіне сәтті проекцияладық:\n",
    "* Как мы видим ниже, мы успешно спроецировали 6 входных токенов из 3D-пространства в 2D-пространство вложений (embedding space):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2369cc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5774bd49",
   "metadata": {},
   "source": [
    "* 2-қадамда, біз сұраныс пен әрбір кілт векторының арасындағы скаляр көбейтіндіні есептеу арқылы нормаланбаған зейін баллдарын есептейміз:\n",
    "\n",
    "***\n",
    "\n",
    "* На следующем, 2-м шаге, мы вычисляем ненормализованные оценки внимания, находя скалярное произведение между вектором запроса и каждым вектором-ключом:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492eebdb",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/15.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0b07ceef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1888)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1] # Python starts index at 0\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c0a972",
   "metadata": {},
   "source": [
    "* Бізде 6 кіріс токен болғандықтан, берілген сұраныс векторы үшін 6 зейін балын есептейміз:\n",
    "\n",
    "***\n",
    "\n",
    "* Поскольку у нас 6 входов, мы получаем 6 оценок внимания для данного вектора-запроса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b72316c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4833, 1.1888, 1.8092, 1.5752, 1.4938, 1.5616])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec65a15c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/16.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78c9d4b",
   "metadata": {},
   "source": [
    "* Содан кейін, 3-қадамда, біз бұрын қолданған `softmax` функциясы арқылы зейін салмақтарын (қосындысы 1-ге тең болатын нормаланған зейін баллдарын) есептейміз.\n",
    "* Бұрынғыдан айырмашылығы, біз енді зейін баллдарын ендіру өлшемінің (embedding dimension) квадрат түбіріне, яғни $\\sqrt{d_k}$ (`d_k**0.5`), бөлу арқылы масштабтаймыз:\n",
    "\n",
    "***\n",
    "\n",
    "* Далее, на 3-м шаге, мы вычисляем веса внимания (нормализованные оценки внимания, сумма которых равна 1), используя `softmax`-функцию, которую мы применяли ранее.\n",
    "* Отличие от предыдущего раза состоит в том, что теперь мы масштабируем оценки внимания, деля их на квадратный корень из размерности вложения (embedding dimension),  \n",
    "$\\sqrt{d_k}$ (`d_k**0.5`):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "edd1bff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1612, 0.1309, 0.2030, 0.1720, 0.1624, 0.1704])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3fd7e7",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/17.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d4aaf0",
   "metadata": {},
   "source": [
    "* **4-қадамда** біз енді 2-ші кіріс сұраныс векторы үшін контекст векторын есептейміз:\n",
    "\n",
    "***\n",
    "\n",
    "* На **4-м шаге** мы теперь вычисляем контекстный вектор для входного вектора-запроса 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f7e3f3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3610, 0.9555])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1214cb",
   "metadata": {},
   "source": [
    "### 2.4.2 Шағын SelfAttention класын іске асыру\n",
    "### 2.4.2 Реализация компактного класса SelfAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec483ae",
   "metadata": {},
   "source": [
    "* Осының бәрін біріктіріп, біз өзіндік зейін механизмін келесідей іске асыра аламыз:\n",
    "\n",
    "* Объединив всё вместе, мы можем реализовать механизм self-attention следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "26614d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3614, 0.9568],\n",
      "        [0.3610, 0.9555],\n",
      "        [0.3616, 0.9569],\n",
      "        [0.3608, 0.9548],\n",
      "        [0.3616, 0.9571],\n",
      "        [0.3611, 0.9558]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e0214",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/18.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcbe5a2",
   "metadata": {},
   "source": [
    "* Жоғарыдағы іске асыруды `PyTorch`-тың `Linear` қабаттарын қолдану арқылы оңтайландыруға болады; егер ығысу бірліктерін (bias units) өшірсек, олар матрицалық көбейтуге эквивалентті болады.\n",
    "* `nn.Linear`-ды біздің қолмен жасаған `nn.Parameter(torch.rand(...)` тәсілімізге қарағанда қолданудың тағы бір үлкен артықшылығы — `nn.Linear`-да салмақтарды инициализациялаудың тиімді схемасы бар, бұл модельді неғұрлым тұрақты **жаттықтыруға** әкеледі.\n",
    "\n",
    "***\n",
    "\n",
    "* Мы можем упростить приведенную выше реализацию, используя линейные слои (`Linear layers`) из `PyTorch`, которые эквивалентны матричному умножению, если мы отключим векторы смещения (bias units).\n",
    "* Еще одно большое преимущество использования `nn.Linear` по сравнению с нашим ручным подходом `nn.Parameter(torch.rand(...)` заключается в том, что `nn.Linear` имеет предпочтительную схему инициализации весов, что приводит к более стабильному обучению модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ce83927d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0758,  0.1225],\n",
      "        [-0.0756,  0.1229],\n",
      "        [-0.0742,  0.1252],\n",
      "        [-0.0763,  0.1216],\n",
      "        [-0.0745,  0.1247],\n",
      "        [-0.0758,  0.1224]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e663fed",
   "metadata": {},
   "source": [
    "* Ескеріңіз, `SelfAttention_v1` және `SelfAttention_v2` әртүрлі шығыс деректер3y береді, себебі олар салмақ матрицалары үшін әртүрлі бастапқы салмақтарды қолданады.\n",
    "* Обратите внимание, что `SelfAttention_v1` и `SelfAttention_v2` выдают разные результаты, поскольку они используют разные начальные веса для весовых матриц.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b13e01c",
   "metadata": {},
   "source": [
    "## 2.5 Себептік (causal) зейін арқылы болашақ сөздерді жасыру\n",
    "## 2.5 Скрытие будущих слов с помощью причинного (causal) внимания "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f59dda",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/19.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44e762d",
   "metadata": {},
   "source": [
    "### 2.5.1 Себептік (causal) зейін құрастыру\n",
    "### 2.5.1 Реализация причинного  (causal) внимания"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f23b6",
   "metadata": {},
   "source": [
    "* Бұл бөлімде біз бұрын откен өзіндік зейін механизмін **себептік (causal) өзіндік зейін механизміне** түрлендіреміз.\n",
    "* Себептік (causal) өзіндік зейін модельдің тізбектегі белгілі бір орынға арналған болжамының болашақ орындарға емес, тек **алдыңғы орындардағы белгілі нәтижелерге ғана тәуелді болуын** қамтамасыз етеді.\n",
    "* Қарапайым сөзбен айтқанда, бұл әрбір келесі сөзді болжауды тек **алдыңғы сөздерге ғана тәуелді** болуын қамтамасыз етеді.\n",
    "* Бұған қол жеткізу үшін біз әрбір берілген токен үшін **болашақ токендерді жасырамыз** (яғни, кіріс мәтінде ағымдағы токеннен кейінгі токендерді):\n",
    "\n",
    "***\n",
    "\n",
    "* В этом разделе мы преобразуем предыдущий механизм self-attention в **причинный (causal) механизм self-attention**.\n",
    "* Причинный (causal) self-attention гарантирует, что предсказание модели для определённой позиции в последовательности будет **зависеть только от известных выходов на предыдущих позициях**, а не от будущих.\n",
    "* Проще говоря, это гарантирует, что предсказание каждого следующего слова **зависит только от предшествующих слов**.\n",
    "* Чтобы достичь этого, для каждого данного токена мы **маскируем будущие токены** (те, которые идут после текущего токена во входном тексте):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4e42b",
   "metadata": {},
   "source": [
    "* **Себептік (causal) өзіндік зейінді** көрсету және іске асыру үшін, алдыңғы бөлімдегі **зейін баллдарымен** және салмақтарымен жұмыс істейміз:\n",
    "\n",
    "***\n",
    "* Чтобы проиллюстрировать и реализовать **причинное self-attention**, мы поработаем с **оценками и весами внимания** из предыдущего раздела:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "52866a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1554, 0.1886, 0.1707, 0.1447, 0.1873, 0.1533],\n",
      "        [0.1582, 0.1882, 0.1678, 0.1460, 0.1854, 0.1545],\n",
      "        [0.1568, 0.1922, 0.1675, 0.1425, 0.1888, 0.1523],\n",
      "        [0.1575, 0.1864, 0.1693, 0.1471, 0.1847, 0.1550],\n",
      "        [0.1567, 0.1916, 0.1679, 0.1429, 0.1884, 0.1525],\n",
      "        [0.1567, 0.1880, 0.1694, 0.1456, 0.1861, 0.1540]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Reuse the query and key weight matrices of the\n",
    "# SelfAttention_v2 object from the previous section for convenience\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs) \n",
    "attn_scores = queries @ keys.T\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac42fea2",
   "metadata": {},
   "source": [
    "* Болашақ зейін салмақтарын жасырудың ең қарапайым жолы — `PyTorch`-тың `tril` функциясы арқылы маска жасау. Бұл маскада негізгі диагональдан төмен орналасқан элементтер (диагональдың өзін қосқанда) 1-ге, ал негізгі диагональдан жоғары орналасқандар 0-ге теңестіріледі:\n",
    "***\n",
    "* Самый простой способ замаскировать будущие веса внимания — это создать маску с помощью функции `tril` из `PyTorch`, в которой элементы ниже главной диагонали (включая саму диагональ) установлены в 1, а элементы выше главной диагонали — в 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1dc69a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f31f41d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1554, 0.1886, 0.1707, 0.1447, 0.1873, 0.1533],\n",
       "        [0.1582, 0.1882, 0.1678, 0.1460, 0.1854, 0.1545],\n",
       "        [0.1568, 0.1922, 0.1675, 0.1425, 0.1888, 0.1523],\n",
       "        [0.1575, 0.1864, 0.1693, 0.1471, 0.1847, 0.1550],\n",
       "        [0.1567, 0.1916, 0.1679, 0.1429, 0.1884, 0.1525],\n",
       "        [0.1567, 0.1880, 0.1694, 0.1456, 0.1861, 0.1540]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ce0ad0",
   "metadata": {},
   "source": [
    "* Содан кейін, диагональдан жоғары орналасқан зейін **баллдарын** нөлге айналдыру үшін біз зейін салмақтарын осы маскаға көбейтеміз:\n",
    "\n",
    "***\n",
    "\n",
    "* Затем мы можем умножить веса внимания на эту маску, чтобы обнулить оценки внимания над диагональю:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c830464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1554, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1582, 0.1882, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1568, 0.1922, 0.1675, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1575, 0.1864, 0.1693, 0.1471, 0.0000, 0.0000],\n",
      "        [0.1567, 0.1916, 0.1679, 0.1429, 0.1884, 0.0000],\n",
      "        [0.1567, 0.1880, 0.1694, 0.1456, 0.1861, 0.1540]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28742dae",
   "metadata": {},
   "source": [
    "* Алайда, егер маска жоғарыдағыдай `softmax`-тан кейін қолданылса, ол `softmax` арқылы жасалған ықтималдық үлестірімін бұзады.\n",
    "* `Softmax` барлық шығыс мәндерінің қосындысы 1-ге тең болуын қамтамасыз етеді.\n",
    "* `Softmax`-тан кейін маскалау шығыс мәндерінің қосындысы қайтадан 1-ге тең болуы үшін оларды қайта нормалауды талап етеді, бұл процесті қиындатады және күтпеген әсерлерге әкелуі мүмкін.\n",
    "***\n",
    "* Однако, если применять маску после `softmax`, как показано выше, это нарушит распределение вероятностей, созданное функцией `softmax`.\n",
    "* `Softmax` гарантирует, что сумма всех выходных значений равна 1.\n",
    "* Маскирование после `softmax` потребовало бы повторной нормализации выходов, чтобы их сумма снова стала равна 1, что усложняет процесс и может привести к нежелательным последствиям."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bfe1ee",
   "metadata": {},
   "source": [
    "* Қатарлардың қосындысы 1-ге тең болуын қамтамасыз ету үшін, біз зейін салмақтарын келесідей нормалай аламыз:\n",
    "***\n",
    "* Чтобы убедиться, что сумма в строках равна 1, мы можем нормализовать веса внимания следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "24d9129e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4567, 0.5433, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3035, 0.3722, 0.3243, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2385, 0.2823, 0.2563, 0.2228, 0.0000, 0.0000],\n",
      "        [0.1849, 0.2260, 0.1981, 0.1687, 0.2223, 0.0000],\n",
      "        [0.1567, 0.1880, 0.1694, 0.1456, 0.1861, 0.1540]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d082c",
   "metadata": {},
   "source": [
    "* Техникалық тұрғыдан себептік зейін механизмін кодтауды аяқтасақ та, жоғарыдағы нәтижеге жетудің тиімдірек тәсілін қысқаша қарастырайық.\n",
    "* Сонымен, диагональдан жоғары орналасқан зейін салмақтарын нөлге айналдырып және нәтижелерді қайта нормалаудың орнына, біз диагональдан жоғары орналасқан нормаланбаған зейін **баллдарын** олар softmax функциясына кірмес бұрын теріс шексіздікпен жасыра аламыз:\n",
    "***\n",
    "* Хотя технически мы уже закончили с реализацией причинного механизма внимания, давайте кратко рассмотрим более эффективный подход для достижения того же результата.\n",
    "* Итак, вместо того чтобы обнулять веса внимания над диагональю и повторно нормализовывать результаты, мы можем замаскировать ненормализованные оценки внимания над диагональю отрицательной бесконечностью, прежде чем они поступят в softmax-функцию:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd39893a",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/20.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e65963",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/21.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8c18c6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1371,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.0999,  0.3455,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.1144,  0.4029,  0.2084,    -inf,    -inf,    -inf],\n",
      "        [ 0.1104,  0.3489,  0.2123,  0.0139,    -inf,    -inf],\n",
      "        [ 0.1163,  0.4005,  0.2143, -0.0136,  0.3769,    -inf],\n",
      "        [ 0.1192,  0.3765,  0.2293,  0.0152,  0.3623,  0.0944]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49cd6d5",
   "metadata": {},
   "source": [
    "* Төменде көріп отырғанымыздай, енді әрбір жолдағы зейін салмақтарының қосындысы қайтадан дұрыс 1-ге тең:\n",
    "***\n",
    "* Как мы видим ниже, теперь сумма весов внимания в каждой строке снова корректно равна 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fb1f6e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4567, 0.5433, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3035, 0.3722, 0.3243, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2385, 0.2823, 0.2563, 0.2228, 0.0000, 0.0000],\n",
      "        [0.1849, 0.2260, 0.1981, 0.1687, 0.2223, 0.0000],\n",
      "        [0.1567, 0.1880, 0.1694, 0.1456, 0.1861, 0.1540]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3388b035",
   "metadata": {},
   "source": [
    "### 2.5.2 Dropout  құралы врқылы қосымша зейін салмақтарын  маскалау\n",
    "### 2.5.2 Маскирование дополнительных весов внимания с помощью dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aea043",
   "metadata": {},
   "source": [
    "* Сонымен қатар, біз **жаттықтыру** кезінде артық үйренуді (overfitting) азайту үшін dropout-ты қолданамыз.\n",
    "* Dropout-ты бірнеше жерде қолдануға болады:\n",
    "    * мысалы, зейін салмақтарын есептегеннен кейін;\n",
    "    * немесе зейін салмақтарын мән векторларына көбейткеннен кейін.\n",
    "* Бұл жерде біз dropout маскасын зейін салмақтарын есептегеннен кейін қолданамыз, себебі бұл жиі кездесетін тәсіл.\n",
    "\n",
    "* Сонымен қатар, осы нақты мысалда біз 50% dropout коэффициентін қолданамыз, бұл зейін салмақтарының жартысын кездейсоқ маскалауды білдіреді. (Кейінірек GPT моделін **жаттықтырған** кезде, біз 0.1 немесе 0.2 сияқты төменірек dropout коэффициентін қолданатын боламыз).\n",
    "\n",
    "***\n",
    "\n",
    "* Кроме того, мы также применяем dropout, чтобы уменьшить переобучение (overfitting) во время обучения.\n",
    "* Dropout можно применять в нескольких местах:\n",
    "    * например, после вычисления весов внимания;\n",
    "    * или после умножения весов внимания на векторы-значения.\n",
    "* Здесь мы будем применять маску dropout после вычисления весов внимания, поскольку это более распространённый подход.\n",
    "\n",
    "* Более того, в этом конкретном примере мы используем коэффициент dropout 50%, что означает случайное маскирование половины весов внимания. (Когда мы будем обучать модель GPT позже, мы будем использовать более низкий коэффициент dropout, например, 0.1 или 0.2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25d4e73",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/22.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4204c5ff",
   "metadata": {},
   "source": [
    "* Егер біз 0.5 (50%) dropout коэффициентін қолдансақ, сақталған (жойылмаған) мәндер сәйкесінше 1/0.5 = 2 коэффициентіне масштабталады.\n",
    "* Масштабтау `1 / (1 - dropout_rate)` формуласы бойынша есептеледі.\n",
    "***\n",
    "* Если мы применим коэффициент dropout 0.5 (50%), то неотброшенные значения будут соответственно отмасштабированы на коэффициент 1/0.5 = 2.\n",
    "* Масштабирование вычисляется по формуле: `1 / (1 - dropout_rate)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a7b7d713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = torch.ones(6, 6)\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9bbb8ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) # dropout rate of 50%\n",
    "example = torch.ones(6, 6) # create a matrix of ones\n",
    "\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "48530f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6070, 0.7443, 0.6487, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5647, 0.5127, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4521, 0.0000, 0.3373, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3760, 0.3389, 0.2913, 0.3723, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a796d547",
   "metadata": {},
   "source": [
    "### 2.5.3 Шағын себептік (causal) өзіндік зейін (self-attention) класын іске асыру\n",
    "### 2.5.3 Реализация компактного класса причинного (causal)  механизма внимания (self-attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38805899",
   "metadata": {},
   "source": [
    "* Енді біз себептік және dropout маскаларын қамтитын, өзіндік зейін механизмінің жұмыс істейтін нұсқасын іске асыруға дайынбыз.\n",
    "* `CausalAttention` класымыз 2-тарауда іске асырған деректерді жүктеуші (data loader) шығаратын батч (топтама) шығыстарын қолдауы үшін, бірнеше кірістен тұратын батчтарды (топтамаларды) өңдейтін кодты іске асыруымыз керек.\n",
    "* Қарапайымдылық үшін, осындай батчтық кірісті модельдеу мақсатында біз кіріс мәтінінің мысалын қайталаймыз:\n",
    "***\n",
    "* Теперь мы готовы реализовать рабочую версию self-attention, включающую причинную маску и маску dropout.\n",
    "* Ещё один момент: необходимо реализовать код для обработки батчей, состоящих из нескольких входов, чтобы наш класс `CausalAttention` поддерживал батчевые выходы, генерируемые загрузчиком данных (data loader), который мы реализовали в главе 2.\n",
    "* Для простоты, чтобы симулировать такой батчевый вход, мы дублируем пример входного текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c191887a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) # 2 inputs with 6 tokens each, and each token has embedding dimension 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "82ae6452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8823, 0.9150, 0.3829],\n",
       "         [0.9593, 0.3904, 0.6009],\n",
       "         [0.2566, 0.7936, 0.9408],\n",
       "         [0.1332, 0.9346, 0.5936],\n",
       "         [0.8694, 0.5677, 0.7411],\n",
       "         [0.4294, 0.8854, 0.5739]],\n",
       "\n",
       "        [[0.8823, 0.9150, 0.3829],\n",
       "         [0.9593, 0.3904, 0.6009],\n",
       "         [0.2566, 0.7936, 0.9408],\n",
       "         [0.1332, 0.9346, 0.5936],\n",
       "         [0.8694, 0.5677, 0.7411],\n",
       "         [0.4294, 0.8854, 0.5739]]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "413563ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8340, -0.3584],\n",
      "         [-0.7842, -0.2061],\n",
      "         [-0.7248, -0.1467],\n",
      "         [-0.6725, -0.1607],\n",
      "         [-0.6983, -0.1409],\n",
      "         [-0.6864, -0.1554]],\n",
      "\n",
      "        [[-0.8340, -0.3584],\n",
      "         [-0.7842, -0.2061],\n",
      "         [-0.7248, -0.1467],\n",
      "         [-0.6725, -0.1607],\n",
      "         [-0.6983, -0.1409],\n",
      "         [-0.6864, -0.1554]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        # For inputs where `num_tokens` exceeds `context_length`, this will result in errors\n",
    "        # in the mask creation further below.\n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n",
    "        # do not exceed `context_length` before reaching this forward method. \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4122b4",
   "metadata": {},
   "source": [
    "* Ескеріңіз, dropout тек **жаттықтыру** кезінде ғана қолданылады, инференс (генерация) кезінде емес.\n",
    "***\n",
    "* Обратите внимание, что dropout применяется только во время обучения, а не во время инференса (генерации)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60166644",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/23.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912dfde8",
   "metadata": {},
   "source": [
    "## 2.6 Жалғыс басты зейін механизмды көп басты зейін механизмына өзгерту \n",
    "## 2.6 Расширение одноголового механизма внимания до многоголового механизма внимания"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c6709",
   "metadata": {},
   "source": [
    "### 2.6.1 Бірнеше жалғыс басты зейін механизмдерды бірге қосу\n",
    "### 2.6.1 Наложение нескольких слоев одноголового внимания"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3591b85f",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/24.webp\" width=\"1000px\">\n",
    "\n",
    "* Көп басты зейін модулін алу үшін біз бірнеше бір басты зейін модульдерін жай ғана қабаттаймыз:\n",
    "***\n",
    "* Мы просто накладываем несколько модулей одноголового внимания, чтобы получить модуль многоголового внимания:\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/25.webp\" width=\"1000px\">\n",
    "\n",
    "* Көп басты зейіннің негізгі идеясы – зейін механизмін әртүрлі, үйретілген сызықтық проекциялармен бірнеше рет (параллельді түрде) іске қосу. Бұл модельге әртүрлі позициялардағы әртүрлі репрезентация кеңістіктерінен алынған ақпаратқа бірлесіп назар аударуға мүмкіндік береді.\n",
    "***\n",
    "Основная идея многоголового внимания заключается в том, чтобы запускать механизм внимания несколько раз (параллельно) с разными, обученными линейными проекциями. Это позволяет модели совместно обращать внимание на информацию из разных подпространств представлений на разных позициях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b1fb4b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8340, -0.3584,  0.7834,  0.6197],\n",
      "         [-0.7842, -0.2061,  0.7593,  0.5032],\n",
      "         [-0.7248, -0.1467,  0.7082,  0.4678],\n",
      "         [-0.6725, -0.1607,  0.6466,  0.4565],\n",
      "         [-0.6983, -0.1409,  0.6726,  0.4510],\n",
      "         [-0.6864, -0.1554,  0.6590,  0.4557]],\n",
      "\n",
      "        [[-0.8340, -0.3584,  0.7834,  0.6197],\n",
      "         [-0.7842, -0.2061,  0.7593,  0.5032],\n",
      "         [-0.7248, -0.1467,  0.7082,  0.4678],\n",
      "         [-0.6725, -0.1607,  0.6466,  0.4565],\n",
      "         [-0.6983, -0.1409,  0.6726,  0.4510],\n",
      "         [-0.6864, -0.1554,  0.6590,  0.4557]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed3a1db",
   "metadata": {},
   "source": [
    "- Жоғарыдағы іске асыруда ендіру өлшемі (embedding  ) 4-ке тең, \n",
    "өйткені біз кілт (query), сұраныс (key), мән (value) векторлары    \n",
    "және контекст (context) векторы үшін ендіру өлшемі ретінде `d_out=2` мәнін қолдандық.    \n",
    "Бізде 2 зейін басы болғандықтан, шығыс ендіру өлшемі 2*2=4 құрайды.\n",
    "***\n",
    "- В реализации выше размерность эмбеддинга равна 4,   \n",
    "потому что мы используем `d_out=2` как размерность эмбеддинга для векторов ключа, запроса и значения,    \n",
    "а также для контекстного вектора. И поскольку у нас 2 головы внимания, выходная размерность эмбеддинга составляет 2*2=4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ca74f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = nn.ModuleList([\n",
    "CausalAttention(d_in, d_out, context_length, 0.0), \n",
    "CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055c3583",
   "metadata": {},
   "source": [
    "### 2.6.2 Салмақтарды бөлу арқылы көп басты зейін механизмын (self-attention) іске асыру\n",
    "### 2.6.2 Реализация многоголового механизма внимания(self-attention) с разделением весов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6fcf5",
   "metadata": {},
   "source": [
    "- Жоғарыдағы код көп басты зейіннің механизмын интуитивті іске асыруы болғанымен (бұрынғы `CausalAttention` бір басты зейін механизмдерді біріктіру арқылы), дәл соған қол жеткізу үшін біз `MultiHeadAttention` деп аталатын жеке класс жаза аламыз.\n",
    "- Біз `MultiHeadAttention` класы үшін біз бір басты зейін механизмдерді бастарын біріктірмейміз.\n",
    "- Оның орнына, біз бірыңғай `W_query`, `W_key` және `W_value` салмақ матрицаларын жасаймыз да, содан кейін оларды әрбір зейін басы үшін жеке матрицаларға бөлеміз:\n",
    "***\n",
    "- Хотя вышеописанное является интуитивно понятной и полнофункциональной реализацией многоголового внимания (обертывающей ранее созданную реализацию одноголового внимания `CausalAttention`), мы можем написать отдельный класс под названием `MultiHeadAttention` для достижения того же результата.\n",
    "- Для этого отдельного класса `MultiHeadAttention` мы не конкатенируем отдельные головы внимания.\n",
    "- Вместо этого мы создаем единые весовые матрицы `W_query`, `W_key` и `W_value`, \n",
    "а затем разделяем их на отдельные матрицы для каждой головы внимания:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dfa69cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2511, 0.2183],\n",
      "         [0.2784, 0.2632],\n",
      "         [0.2814, 0.3009],\n",
      "         [0.2694, 0.3244],\n",
      "         [0.2780, 0.3152],\n",
      "         [0.2728, 0.3183]],\n",
      "\n",
      "        [[0.2511, 0.2183],\n",
      "         [0.2784, 0.2632],\n",
      "         [0.2814, 0.3009],\n",
      "         [0.2694, 0.3244],\n",
      "         [0.2780, 0.3152],\n",
      "         [0.2728, 0.3183]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "        self.register_buffer(                                          \n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`, \n",
    "        # this will result in errors in the mask creation further below. \n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n",
    "        # do not exceed `context_length` before reaching this forward method.\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b14bed0",
   "metadata": {},
   "source": [
    "- Ескеріңіз, жоғарыда көрсетілген нұсқа `MultiHeadAttentionWrapper`-дың қайта жазылған нұсқасы болып табылады.\n",
    "- Кездейсоқ салмақ инициализациялары әртүрлі болғандықтан, нәтижелік шығыс сәл өзгеше көрінеді, бірақ екеуі де алдағы тарауларда іске асыратын GPT класында қолдануға болатын толық функционалды іске асырулар болып табылады.\n",
    "***\n",
    "- Обратите внимание, что вышеприведенное, по сути, является переписанной  версией `MultiHeadAttentionWrapper`.\n",
    "- Итоговый результат выглядит немного иначе, поскольку различаются случайные инициализации весов, но оба варианта являются полностью функциональными реализациями, которые можно использовать в классе GPT, который мы будем реализовывать в следующих главах."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01601f42",
   "metadata": {},
   "source": [
    "**Шығыс өлшемдері туралы ескертпе**\n",
    "- Жоғарыдағы `MultiHeadAttention`-да  бұрынғы `MultiHeadAttentionWrapper` класындағыдай параметрді пайдалану үшін `d_out=2` деп қойдым.\n",
    "- `MultiHeadAttentionWrapper` конкатенацияға байланысты шығыс басының өлшемін `d_out * num_heads` (яғни, `2*2 = 4`) ретінде қайтарады.\n",
    "- Алайда, `MultiHeadAttention` класы (оны пайдаланушыға ыңғайлы ету үшін) шығыс басының өлшемін тікелей `d_out` арқылы басқаруға мүмкіндік береді; бұл дегеніміз, егер біз `d_out = 2` деп қойсақ, шығыс басының өлшемі бастар санына қарамастан 2 болады.\n",
    "***\n",
    "**Примечание о выходных размерностях**\n",
    "- В приведённом выше `MultiHeadAttention` я использовал `d_out=2`, чтобы применить ту же настройку, что и в классе `MultiHeadAttentionWrapper` ранее.\n",
    "- `MultiHeadAttentionWrapper` из-за конкатенации возвращает выходную размерность головы `d_out * num_heads` (т.е., `2*2 = 4`).\n",
    "- Однако класс `MultiHeadAttention` (чтобы сделать его более удобным для пользователя) позволяет нам напрямую контролировать выходную размерность головы через `d_out`; это означает, что если мы установим `d_out = 2`, выходная размерность головы будет равна 2, независимо от количества голов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1215b7cd",
   "metadata": {},
   "source": [
    "- Ескеріңіз, сонымен қатар біз жоғарыдағы `MultiHeadAttention` класына сызықтық проекция қабатын (`self.out_proj`) қостық. Бұл – өлшемдерді өзгертпейтін қарапайым сызықтық түрлендіру. LLM іске асыруларында мұндай проекция қабатын қолдану стандартты тәжірибе болып саналады, бірақ ол қатаң түрде міндетті емес.\n",
    "***\n",
    "- Обратите внимание, что мы также добавили слой линейной проекции (`self.out_proj`) в приведённый выше класс `MultiHeadAttention`. Это простое линейное преобразование, которое не изменяет размерности. Использование такого проекционного слоя является стандартной практикой при реализации LLM, но оно не является строго необходимым."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c013e",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/26.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db92cad7",
   "metadata": {},
   "source": [
    "- Ескеріңіз, егер сізді жоғарыдағының ықшам әрі тиімді іске асырылуы қызықтырса, PyTorch-тағы `torch.nn.MultiheadAttention` класын да қарастыруыңызға болады.\n",
    "***\n",
    "- Обратите внимание, что если вас интересует компактная и эффективная реализация вышеизложенного, вы также можете рассмотреть класс `torch.nn.MultiheadAttention` в PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749f21d2",
   "metadata": {},
   "source": [
    "- `MultiheadAttention` механизмысындағы зейін баллдарының кадам кадаммен есептеу әдісі\n",
    "- Пошаговое вычисление баллов внимания в `MultiheadAttention` механизме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "121d1600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1608,  0.5821],\n",
       "        [-0.3328,  0.2560],\n",
       "        [-0.1634, -0.0290],\n",
       "        [-0.0162,  0.1699],\n",
       "        [-0.3129,  0.2260],\n",
       "        [-0.0973,  0.2799]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "keys = W_key(inputs)\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "48479dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8823, 0.9150, 0.3829],\n",
       "        [0.9593, 0.3904, 0.6009],\n",
       "        [0.2566, 0.7936, 0.9408],\n",
       "        [0.1332, 0.9346, 0.5936],\n",
       "        [0.8694, 0.5677, 0.7411],\n",
       "        [0.4294, 0.8854, 0.5739]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1c7f402b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "540dc266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "50603028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1608,  0.5821],\n",
       "        [-0.3328,  0.2560],\n",
       "        [-0.1634, -0.0290],\n",
       "        [-0.0162,  0.1699],\n",
       "        [-0.3129,  0.2260],\n",
       "        [-0.0973,  0.2799]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2969bd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=1\n",
    "num_tokens =6   \n",
    "num_heads =2\n",
    "head_dim =1\n",
    "keys2 = keys.view(b, num_tokens, num_heads, head_dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ab7c93b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 2, 1])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e92cca4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1608],\n",
       "          [ 0.5821]],\n",
       "\n",
       "         [[-0.3328],\n",
       "          [ 0.2560]],\n",
       "\n",
       "         [[-0.1634],\n",
       "          [-0.0290]],\n",
       "\n",
       "         [[-0.0162],\n",
       "          [ 0.1699]],\n",
       "\n",
       "         [[-0.3129],\n",
       "          [ 0.2260]],\n",
       "\n",
       "         [[-0.0973],\n",
       "          [ 0.2799]]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "426b225c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1608],\n",
       "         [-0.3328],\n",
       "         [-0.1634],\n",
       "         [-0.0162],\n",
       "         [-0.3129],\n",
       "         [-0.0973]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys2[:, :, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "36fc6903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5821],\n",
       "         [ 0.2560],\n",
       "         [-0.0290],\n",
       "         [ 0.1699],\n",
       "         [ 0.2260],\n",
       "         [ 0.2799]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys2[:, :, 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "664d4133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1608,  0.5821],\n",
       "        [-0.3328,  0.2560],\n",
       "        [-0.1634, -0.0290],\n",
       "        [-0.0162,  0.1699],\n",
       "        [-0.3129,  0.2260],\n",
       "        [-0.0973,  0.2799]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f67600a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 6, 1])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys3 = keys2.transpose(1, 2)\n",
    "keys3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1c768b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1608],\n",
       "          [-0.3328],\n",
       "          [-0.1634],\n",
       "          [-0.0162],\n",
       "          [-0.3129],\n",
       "          [-0.0973]],\n",
       "\n",
       "         [[ 0.5821],\n",
       "          [ 0.2560],\n",
       "          [-0.0290],\n",
       "          [ 0.1699],\n",
       "          [ 0.2260],\n",
       "          [ 0.2799]]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "649529b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1608, -0.3328, -0.1634, -0.0162, -0.3129, -0.0973]],\n",
       "\n",
       "         [[ 0.5821,  0.2560, -0.0290,  0.1699,  0.2260,  0.2799]]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys3.transpose(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "28df43f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "queries = W_query(inputs)\n",
    "queries = queries.view(b, num_tokens, num_heads, head_dim)\n",
    "queries = queries.transpose(1, 2)\n",
    "attn_scores = queries @ keys3.transpose(2, 3) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
