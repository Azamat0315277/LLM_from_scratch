{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4d903b9",
   "metadata": {},
   "source": [
    "# GPT-2 архитектурасын Llama 2 архитектурасына түрлендіру\n",
    "# Преобразование архитектуры GPT - 2  в Llama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6359df71",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/gpt2-to-llama2-llama3.webp?1\" width=1000px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb583502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"torch\",            # to implement the model\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f421a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d133d7",
   "metadata": {},
   "source": [
    "##   LayerNorm қабатын RMSNorm қабатына ауыстыру\n",
    "##  Заменить слой LayerNorm на слой RMSNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b54849",
   "metadata": {},
   "source": [
    "* Алдымен біз LayerNorm қабатын Root Mean Square Layer Normalization (RMSNorm) қабатымен ауыстырамыз.\n",
    "* LayerNorm кірістерді орташа мән мен дисперсия арқылы қалыпқа келтіреді, ал RMSNorm тек квадраттық орташа мәнді (root mean square) пайдаланады, бұл есептеу тиімділігін арттырады.\n",
    "* RMSNorm операциясы:    \n",
    "$x$ — кіріс,    \n",
    "$\\gamma$ — үйренілетін параметр (вектор),   \n",
    "$\\epsilon$ — нөлге бөлу қатесінен сақтану үшін қолданылатын кіші тұрақты шама:\n",
    "\n",
    "$$y_i = \\frac{x_i}{\\text{RMS}(x)} \\gamma_i, \\quad \\text{мұндағы} \\quad \\text{RMS}(x) = \\sqrt{\\epsilon + \\frac{1}{n} \\sum x_i^2}$$\n",
    "---\n",
    "* Сначала мы заменяем слой LayerNorm на Root Mean Square Layer Normalization (RMSNorm).\n",
    "* LayerNorm нормализует входы, используя среднее и дисперсию, тогда как RMSNorm использует только квадратный корень из среднего квадрата (root mean square), что повышает вычислительную эффективность.\n",
    "* Операция RMSNorm определяется следующим образом:   \n",
    "$x$ — вход,   \n",
    "$\\gamma$ — обучаемый параметр (вектор),  \n",
    "$\\epsilon$ — малая константа, предотвращающая деление на ноль:\n",
    "\n",
    "$$y_i = \\frac{x_i}{\\text{RMS}(x)} \\gamma_i, \\quad \\text{где} \\quad \\text{RMS}(x) = \\sqrt{\\epsilon + \\frac{1}{n} \\sum x_i^2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a17565c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 3\n",
    "#####################################\n",
    "\n",
    "# class LayerNorm(nn.Module):\n",
    "#     def __init__(self, emb_dim):\n",
    "#         super().__init__()\n",
    "#         self.eps = 1e-5\n",
    "#         self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "#         self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mean = x.mean(dim=-1, keepdim=True)\n",
    "#         var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "#         norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "#         return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.emb_dim = emb_dim\n",
    "        self.weight = nn.Parameter(torch.ones(emb_dim)).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        means = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        x_normed = x * torch.rsqrt(means + self.eps)\n",
    "        return (x_normed * self.weight).to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af56e61d",
   "metadata": {},
   "source": [
    "##  GELU функциясын SiLU активациясына ауыстыру\n",
    "##  Заменить функцию активации GELU на SiLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8556ad",
   "metadata": {},
   "source": [
    "* Llama GELU орнына SiLU (Swish) активация функциясын пайдаланады\n",
    "$$\n",
    "\\text{silu}(x) = x \\cdot \\sigma(x), \\quad \\text{мұндағы} \\quad \\sigma(x) \\text{ — логистикалық сигмоида.}\n",
    "$$\n",
    "---\n",
    "* В модели Llama используется функция активации SiLU (вместо GELU), также известная как функция Swish:\n",
    "\n",
    "$$\n",
    "\\text{silu}(x) = x \\cdot \\sigma(x), \\quad \\text{где} \\quad \\sigma(x) \\text{ — логистическая сигмоида.}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "569cbd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 3\n",
    "#####################################\n",
    "\n",
    "# class GELU(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return 0.5 * x * (1 + torch.tanh(\n",
    "#             torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "#             (x + 0.044715 * torch.pow(x, 3))\n",
    "#         ))\n",
    "\n",
    "\n",
    "class SiLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda6a3d",
   "metadata": {},
   "source": [
    "##  FeedForward модулін жаңарту\n",
    "##  Обновление модуля FeedForward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e61398f",
   "metadata": {},
   "source": [
    "* Шын мәнінде, Llama SiLU функциясының “Gated Linear Unit” (GLU) нұсқасы болып табылатын SwiGLU-ды қолданады, бұл `FeedForward` модулінің құрылымын сәл өзгеше етеді.\n",
    "* SwiGLU feedforward қабатында **gating** (қақпалау) механизмін қолданады, оның формуласы:\n",
    "\n",
    "$$\\text{SwiGLU}(x) = \\text{SiLU}(\\text{Linear}_1(x)) * (\\text{Linear}_2(x))$$\n",
    "\n",
    "* Мұнда $\\text{Linear}_1$ және $\\text{Linear}_2$ — екі сызықтық қабат, ал $*$ — элемент бойынша көбейту (element-wise multiplication).\n",
    "* Үшінші сызықтық қабат — $\\text{Linear}_3$ — осы gated активациядан кейін қолданылады.\n",
    "---\n",
    "* На самом деле, Llama использует вариант функции SiLU под названием **SwiGLU**, которая является модификацией “Gated Linear Unit” (GLU). Это приводит к немного другой структуре модуля `FeedForward`.\n",
    "* SwiGLU применяет **механизм «врат» (gating)** в полносвязном слое, формула выглядит так:\n",
    "\n",
    "$$\\text{SwiGLU}(x) = \\text{SiLU}(\\text{Linear}_1(x)) * (\\text{Linear}_2(x))$$\n",
    "\n",
    "* Здесь $\\text{Linear}_1$ и $\\text{Linear}_2$ — это два линейных слоя, а $*$ обозначает покомпонентное (element-wise) умножение.\n",
    "* Третий линейный слой, $\\text{Linear}_3$, применяется после этой gated-активации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "175a4ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 3\n",
    "#####################################\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self, cfg):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.Sequential(\n",
    "#             nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "#             GELU(),\n",
    "#             nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.layers(x)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.silu = SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = self.silu(x_fc1) * x_fc2\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc247d2",
   "metadata": {},
   "source": [
    "* Айта кету керек, біз жоғарыда `dtype=cfg[\"dtype\"]` параметрін де қостық. Жадты үнемдеу үшін бұл модельді кейінірек төмен дәлдіктегі форматтарда (lower precision) тікелей жүктеуге мүмкіндік береді (модельді бастапқы 32-бит форматында жасап, кейін түрлендірудің орнына).\n",
    "* Сондай-ақ, біз `bias=False` деп орнаттық, себебі Llama модельдері **bias** бірліктерін қолданбайды.\n",
    "---\n",
    "* Обратите внимание, что мы также добавили параметр `dtype=cfg[\"dtype\"]`, который позволит нам загружать модель напрямую в форматах с пониженной точностью, чтобы уменьшить использование памяти (вместо создания модели в исходном 32-битном формате и последующего преобразования).\n",
    "* Мы также установили `bias=False`, поскольку модель Llama не использует **bias**-элементы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c413138a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdc4b1f8",
   "metadata": {},
   "source": [
    "* GPT моделінде позициялық эмбеддингтер келесідей түрде жүзеге асырылады:\n",
    "\n",
    "```python\n",
    "self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "```\n",
    "\n",
    "* Дәстүрлі абсолюттік позициялық эмбеддингтерге қарағанда, **Llama** rotary position embeddings (RoPE) әдісін қолданады, бұл модельге **абсолюттік және салыстырмалы позициялық ақпаратты бір уақытта** үйренуге мүмкіндік береді.\n",
    "* RoPE туралы негізгі мақала: [RoFormer: Enhanced Transformer with Rotary Position Embedding (2021)](https://arxiv.org/abs/2104.09864).\n",
    "* RoPE екі эквивалентті тәсілмен жүзеге асырылуы мүмкін: *split-halves* нұсқасы және *interleaved even/odd* нұсқасы арқылы;\n",
    "* Бұл кодта RoPE-тің *split-halves* тәсілі қолданылады.\n",
    "---\n",
    "* В модели GPT позиционные эмбеддинги реализованы следующим образом:\n",
    "\n",
    "```python\n",
    "self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "```\n",
    "\n",
    "* В отличие от традиционных абсолютных позиционных эмбеддингов, **Llama** использует **rotary position embeddings (RoPE)**, которые позволяют одновременно учитывать **абсолютную и относительную позиционную информацию**.\n",
    "* Основная статья по RoPE: [RoFormer: Enhanced Transformer with Rotary Position Embedding (2021)](https://arxiv.org/abs/2104.09864).\n",
    "* RoPE можно реализовать двумя эквивалентными способами: *split-halves* и *interleaved even/odd*;\n",
    "* В этом коде используется метод *split-halves*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71403d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_rope_params(head_dim, theta_base=10_000, context_length=4096):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))\n",
    "\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length)\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions.unsqueeze(1) * inv_freq.unsqueeze(0)  # Shape: (context_length, head_dim // 2)\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
    "\n",
    "    # Precompute sine and cosine\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "def compute_rope(x, cos, sin):\n",
    "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # Split x into first half and second half\n",
    "    x1 = x[..., : head_dim // 2]  # First half\n",
    "    x2 = x[..., head_dim // 2 :]  # Second half\n",
    "\n",
    "    # Adjust sin and cos shapes\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply the rotary transformation\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fabd92",
   "metadata": {},
   "source": [
    "* Төменде `q` және `k` тензорларына RoPE қолданудың мысалы келтірілген:\n",
    "---\n",
    "* Ниже приведён пример применения RoPE к тензорам `q` и `k`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a76500f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "batch_size = 2\n",
    "context_len = 5\n",
    "num_heads = 4\n",
    "head_dim = 16\n",
    "\n",
    "# Instantiate RoPE parameters\n",
    "cos, sin = precompute_rope_params(head_dim=head_dim, context_length=context_len)\n",
    "\n",
    "# Dummy query and key tensors\n",
    "torch.manual_seed(123)\n",
    "queries = torch.randn(batch_size, num_heads, context_len, head_dim)\n",
    "keys = torch.randn(batch_size, num_heads, context_len, head_dim)\n",
    "\n",
    "# Apply rotary position embeddings\n",
    "queries_rot = compute_rope(queries, cos, sin)\n",
    "keys_rot = compute_rope(keys, cos, sin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce250d",
   "metadata": {},
   "source": [
    "## RoPE-ті MultiHeadAttention модуліне қосу\n",
    "## Добавление RoPE в модуль MultiHeadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461928e6",
   "metadata": {},
   "source": [
    "* Маңыздысы — GPT позициялық эмбеддингтерді **кіріс векторларына** қолданады, ал Llama позициялық ақпаратты **self-attention механизміндегі query және key векторларына айналдыру (rotation)** арқылы енгізеді.\n",
    "* Мұнда біз `MultiHeadAttention` класын тиісті RoPE кодымен өзгертеміз.\n",
    "* Сонымен қатар, `qkv_bias` параметрін алып тастап, `bias=False` мәнін  орнатамыз.\n",
    "* Сондай-ақ, кейін модельді төмен дәлдіктегі форматта (lower precision) инициализациялау үшін `dtype` параметрін қосамыз.\n",
    "---\n",
    "* Важно отметить, что GPT применяет позиционные эмбеддинги **к входным векторам**, тогда как Llama добавляет позиционную информацию, **вращая (rotating)** векторы query и key прямо внутри механизма самовнимания (self-attention).\n",
    "* Здесь мы модифицируем класс `MultiHeadAttention`, добавляя соответствующий код для RoPE.\n",
    "* Кроме того, мы удаляем параметр `qkv_bias` и  задаём значение `bias=False`.\n",
    "* Также мы добавляем параметр `dtype`, чтобы в дальнейшем можно было инициализировать модель в формате с пониженной точностью (lower precision).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1ffc490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 2\n",
    "#####################################\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, num_heads, dtype=None):  # ,dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        # Set bias=False and dtype=dtype for all linear layers below\n",
    "        ###########################################################################\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)  # Linear layer to combine head outputs\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        cos, sin = precompute_rope_params(head_dim=self.head_dim, context_length=context_length)\n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin)\n",
    "        ###########################################################################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        keys = compute_rope(keys, self.cos, self.sin)\n",
    "        queries = compute_rope(queries, self.cos, self.sin)\n",
    "        ###########################################################################\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        # attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955f7933",
   "metadata": {},
   "source": [
    "* Төменде `MultiHeadAttention` модулін мысалын  қолдану үлгісі көрсетілген:\n",
    "---\n",
    "* Ниже приведён пример использования модуля `MultiHeadAttention` на примере входных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d72a8c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "batch_size = 1\n",
    "context_len = 100\n",
    "max_context_len = 4096\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "\n",
    "\n",
    "example_batch = torch.randn((batch_size, context_len, embed_dim))\n",
    "\n",
    "mha = MultiHeadAttention(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=max_context_len,\n",
    "    num_heads=num_heads\n",
    ")\n",
    "\n",
    "mha(example_batch)\n",
    "\n",
    "del mha  # delete to free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0030041b",
   "metadata": {},
   "source": [
    "## TransformerBlock модулін жаңарту\n",
    "## Обновление модуля TransformerBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1393a1e5",
   "metadata": {},
   "source": [
    "Енді біз жоғарыда жүзеге асырған кодты пайдалану үшін `TransformerBlock` модулін жаңарта аламыз:\n",
    "  * **LayerNorm** орнына **RMSNorm** қолданамыз\n",
    "  * **Dropout** қабатын алып тастаймыз\n",
    "  * `qkv_bias` параметрін жоямыз\n",
    "  * `dtype` параметрін қосамыз\n",
    "---\n",
    "Теперь мы можем обновить модуль `TransformerBlock`, чтобы использовать код, реализованный выше:\n",
    "  * заменяем **LayerNorm** на **RMSNorm**\n",
    "  * удаляем **Dropout**\n",
    "  * убираем параметр `qkv_bias`\n",
    "  * добавляем параметр `dtype`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c074696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dtype=cfg[\"dtype\"]  # NEW\n",
    "            # dropout=cfg[\"drop_rate\"],\n",
    "            # qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        # self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        # self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm1 = RMSNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = RMSNorm(cfg[\"emb_dim\"])\n",
    "        ###########################################################################\n",
    "\n",
    "        # self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        # x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        # x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46949abc",
   "metadata": {},
   "source": [
    "##  Модель класын жаңарту\n",
    "##  Обновление класса модели\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2ae137",
   "metadata": {},
   "source": [
    "* Біздің Llama моделіміз дерлік дайын, тек `TransformerBlock` айналасындағы модель кодын жаңарту қажет:\n",
    "  * Енді RoPE эмбеддингтерін қолданғандықтан, **абсолюттік позициялық эмбеддингтерді** алып тастаймыз\n",
    "  * **LayerNorm** орнына **RMSNorm** қолданамыз\n",
    "  * **Dropout** қабатын алып тастаймыз\n",
    "  * **dtype** параметрін қосамыз\n",
    "---\n",
    "* Наша модель Llama почти завершена — осталось лишь обновить код модели вокруг `TransformerBlock`:\n",
    "  * удаляем **абсолютные позиционные эмбеддинги**, так как теперь используем RoPE\n",
    "  * заменяем **LayerNorm** на **RMSNorm**\n",
    "  * удаляем **Dropout**\n",
    "  * добавляем параметр **dtype**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31f13d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GPTModel(nn.Module):\n",
    "class Llama2Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "        # self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        # self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        # self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
    "        ###########################################################################\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        # batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        # pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds  # + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        # x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaec7ec",
   "metadata": {},
   "source": [
    "##  Модельді инициализациялау\n",
    "##  Инициализация модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd6f3221",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA2_CONFIG_7B = {\n",
    "    \"vocab_size\": 32000,     # Vocabulary size\n",
    "    \"context_length\": 4096,  # Context length\n",
    "    \"emb_dim\": 4096,         # Embedding dimension\n",
    "    \"n_heads\": 32,           # Number of attention heads\n",
    "    \"n_layers\": 32,          # Number of layers\n",
    "    \"hidden_dim\": 11008,     # NEW: Size of the intermediate dimension in FeedForward\n",
    "    \"dtype\": torch.bfloat16  # NEW: Lower-precision dtype to reduce memory usage\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "449dca18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 6,738,415,616\n"
     ]
    }
   ],
   "source": [
    "model = Llama2Model(LLAMA2_CONFIG_7B)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286bf9bf",
   "metadata": {},
   "source": [
    "* Жоғарыда көрсетілгендей, модельде **6.7 миллиард параметр** бар (әдетте оны жуықтап **7B модель** деп атайды).\n",
    "* Сонымен қатар, төмендегі код арқылы осы модельдің жады көлеміне қойылатын талаптарын есептеуге болады:\n",
    "---\n",
    "* Как показано выше, модель содержит **6,7 миллиарда параметров** (обычно округляется и называется **моделью 7B**).\n",
    "* Кроме того, с помощью приведённого ниже кода можно вычислить требования к памяти для этой модели:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6345f325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (PyTorch default): 52.33 GB\n",
      "bfloat16: 26.17 GB\n"
     ]
    }
   ],
   "source": [
    "def model_memory_size(model, input_dtype=torch.float32):\n",
    "    total_params = 0\n",
    "    total_grads = 0\n",
    "    for param in model.parameters():\n",
    "        # Calculate total number of elements per parameter\n",
    "        param_size = param.numel()\n",
    "        total_params += param_size\n",
    "        # Check if gradients are stored for this parameter\n",
    "        if param.requires_grad:\n",
    "            total_grads += param_size\n",
    "\n",
    "    # Calculate buffer size (non-parameters that require memory)\n",
    "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
    "\n",
    "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
    "    # We assume parameters and gradients are stored in the same type as input dtype\n",
    "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
    "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
    "\n",
    "    # Convert bytes to gigabytes\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    return total_memory_gb\n",
    "\n",
    "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
    "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba9ae06",
   "metadata": {},
   "source": [
    "# Llama 2 архитектурасын Llama 3.2 архитектурасына түрлендіру\n",
    "# Преобразование архитектуры Llama 2  в Llama 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc739ce2",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/gpt2-to-llama2-llama3.webp?1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e172b228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.8.0\n",
      "torch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"tiktoken\",         # to implement the tokenizer\n",
    "    \"torch\",            # to implement the model\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f6861",
   "metadata": {},
   "source": [
    "##  RoPE жаңарту\n",
    "##  Модифицикация RoPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e6df8",
   "metadata": {},
   "source": [
    "* **Llama 3** моделі **Llama 2** сияқты **rotary position embeddings (RoPE)** әдісін қолданады (толық түсіндірме үшін [RoPE мақаласын](https://arxiv.org/abs/2104.09864) қараңыз).\n",
    "* Алайда RoPE параметрлерінде аздаған айырмашылықтар бар:\n",
    "\n",
    "  * **Llama 3** енді **8,192 токенге дейін** қолдайды — бұл **Llama 2**-дегі (4,096) саннан екі есе көп.\n",
    "  * Төмендегі формуладағы **RoPE $\\theta$** айнымалысының базалық мәні **10,000**-нан (**Llama 2**) **500,000**-ға (**Llama 3**) арттырылған ([RoPE мақаласынан](https://arxiv.org/abs/2104.09864) бейімделген формула):\n",
    "\n",
    "$$\\Theta = \\left\\{\\theta_i = \\text{base}^{\\frac{-2(i-1)}{d}}, i \\in \\left[1, 2, ..., d/2\\right]\\right\\}$$\n",
    "\n",
    "* Бұл $\\theta$ мәндері — **айналдыру матрицасындағы бұрыштарды анықтайтын алдын ала берілген параметрлер**, мұнда $d$ — эмбеддинг кеңістігінің өлшемі.\n",
    "* **Базаны 10,000-нан 500,000-ға арттыру** жиіліктердің (немесе айналу бұрыштарының) өлшемдерін  баяу өзгеруіне әкеледі, яғни жоғары өлшемдер бұрынғыға қарағанда **үлкен бұрыштармен** байланысады (бұл — жиіліктердің \"декомпрессиясы\").\n",
    "---\n",
    "* **Llama 3** использует **rotary position embeddings (RoPE)**, аналогично **Llama 2** (подробное объяснение можно найти в [статье о RoPE](https://arxiv.org/abs/2104.09864)).\n",
    "* Однако в настройках RoPE есть несколько отличий:\n",
    "\n",
    "  * **Llama 3** теперь поддерживает до **8 192 токенов**, что вдвое больше, чем в **Llama 2** (4 096).\n",
    "  * Базовое значение параметра **RoPE $\\theta$** увеличено с **10 000** (в Llama 2) до **500 000** (в Llama 3) в следующем уравнении (адаптированном из [статьи RoPE](https://arxiv.org/abs/2104.09864)):\n",
    "\n",
    "$$\\Theta = \\left\\{\\theta_i = \\text{base}^{\\frac{-2(i-1)}{d}}, i \\in \\left[1, 2, ..., d/2\\right]\\right\\}$$\n",
    "\n",
    "* Эти значения $\\theta$ — это набор заранее определённых параметров, которые определяют углы вращения в роторной матрице, где $d$ — размерность пространства эмбеддингов.\n",
    "* **Увеличение базы с 10 000 до 500 000** приводит к тому, что частоты (или углы вращения) уменьшаются медленнее по измерениям, то есть **высшие измерения** будут связаны с **большими углами**, чем раньше (по сути, это «декомпрессия» частот).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "677ed13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def precompute_rope_params(head_dim, theta_base=10_000, context_length=4096, freq_config=None):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))\n",
    "\n",
    "    ################################ NEW ###############################################\n",
    "    # Frequency adjustments\n",
    "    if freq_config is not None:\n",
    "        low_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"low_freq_factor\"]\n",
    "        high_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"high_freq_factor\"]\n",
    "\n",
    "        wavelen = 2 * torch.pi / inv_freq\n",
    "\n",
    "        inv_freq_llama = torch.where(\n",
    "            wavelen > low_freq_wavelen, inv_freq / freq_config[\"factor\"], inv_freq\n",
    "        )\n",
    "\n",
    "        smooth_factor = (freq_config[\"original_context_length\"] / wavelen - freq_config[\"low_freq_factor\"]) / (\n",
    "            freq_config[\"high_freq_factor\"] - freq_config[\"low_freq_factor\"]\n",
    "        )\n",
    "\n",
    "        smoothed_inv_freq = (\n",
    "            (1 - smooth_factor) * (inv_freq / freq_config[\"factor\"]) + smooth_factor * inv_freq\n",
    "        )\n",
    "\n",
    "        is_medium_freq = (wavelen <= low_freq_wavelen) & (wavelen >= high_freq_wavelen)\n",
    "        inv_freq_llama = torch.where(is_medium_freq, smoothed_inv_freq, inv_freq_llama)\n",
    "        inv_freq = inv_freq_llama\n",
    "    ####################################################################################\n",
    "\n",
    "\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length)\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions.unsqueeze(1) * inv_freq.unsqueeze(0)  # Shape: (context_length, head_dim // 2)\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
    "\n",
    "    # Precompute sine and cosine\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7590fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate RoPE parameters\n",
    "\n",
    "llama_2_context_len = 4096\n",
    "llama_3_context_len = 8192\n",
    "\n",
    "llama_2_theta_base = 10_000\n",
    "llama_3_theta_base = 500_000\n",
    "\n",
    "# Settings\n",
    "batch_size = 2\n",
    "num_heads = 4\n",
    "head_dim = 16\n",
    "\n",
    "# Instantiate RoPE parameters\n",
    "cos, sin = precompute_rope_params(\n",
    "    head_dim=head_dim,\n",
    "    theta_base=llama_3_theta_base,\n",
    "    context_length=llama_3_context_len\n",
    ")\n",
    "\n",
    "# Dummy query and key tensors\n",
    "torch.manual_seed(123)\n",
    "queries = torch.randn(batch_size, num_heads, llama_3_context_len, head_dim)\n",
    "keys = torch.randn(batch_size, num_heads, llama_3_context_len, head_dim)\n",
    "\n",
    "# Apply rotary position embeddings\n",
    "queries_rot = compute_rope(queries, cos, sin)\n",
    "keys_rot = compute_rope(keys, cos, sin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1926d7",
   "metadata": {},
   "source": [
    "## Топталған сұрау арқылы назар аудару (Grouped-query attention)\n",
    "## Группированное внимание по запросам (Grouped-query attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8cc42",
   "metadata": {},
   "source": [
    "* Бұл бөлімде біз **multi-head attention (MHA)** механизмін **grouped-query attention (GQA)** деп аталатын балама тәсілмен ауыстырамыз.\n",
    "* Қысқаша айтқанда, **GQA** — бұл **есептеу және параметр тұрғысынан тиімдірек** MHA нұсқасы.\n",
    "* **GQA**-да key және value проекцияларының санын азайту үшін оларды бірнеше attention head арасында ортақ пайдаланамыз.\n",
    "* Әрбір attention head өзіне тән **query** векторына ие, бірақ олардың барлығы **бір топтағы key және value** векторларына назар аударады.\n",
    "* Төменде 2 key-value тобы (kv-groups) бар **GQA** мысалы көрсетілген:\n",
    "---\n",
    "* В этом разделе мы заменяем механизм **multi-head attention (MHA)** на альтернативный — **grouped-query attention (GQA)**.\n",
    "* Вкратце, **GQA** можно рассматривать как **более вычислительно и параметрически эффективную** версию MHA.\n",
    "* В **GQA** уменьшается количество проекций key и value за счёт их **совместного использования несколькими головами внимания**.\n",
    "* Каждая голова внимания по-прежнему имеет свой собственный **query**, но все они обращаются к **одной группе ключей и значений**.\n",
    "* Ниже представлена иллюстрация **GQA** с двумя группами key-value (kv-groups):\n",
    "\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/grouped-query-attention.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4295f30f",
   "metadata": {},
   "source": [
    "* GQA-ның негізгі идеясы — кілт-мағына (key-value) жұптарына қатысатын бірегей сұраныс топтарының (query groups) санын азайту. Бұл матрица көбейтулерінің кейбірінің өлшемін және MHA-дегі параметрлер санын азайтып, модель өнімділігін айтарлықтай төмендетпей тиімділікті арттырады.\n",
    "* GQA коды MHA-ға өте ұқсас (төменде \"NEW\" деп белгіленген бөлімдер арқылы өзгертулер көрсетілген).\n",
    "* Қысқаша айтқанда, GQA-дағы негізгі өзгеріс — әрбір сұраныс тобы оған сәйкес келетін бас (head) санына сәйкес келуі үшін қайталануы керек, бұл төменде көрсетілгендей жүзеге асырылады.\n",
    "---\n",
    "* Основная идея GQA заключается в сокращении количества уникальных групп запросов (query groups), обращающихся к парам ключ-значение (key-value), что уменьшает размер некоторых матричных умножений и количество параметров в MHA, не снижая при этом существенно качество модели.\n",
    "* Код GQA очень похож на MHA (изменения выделены ниже в разделах \"NEW\").\n",
    "* Вкратце, главное изменение в GQA состоит в том, что каждая группа запросов должна быть повторена столько раз, сколько голов (heads) с ней связано, как показано в реализации ниже.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4999e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self, d_in, d_out, num_heads,\n",
    "            num_kv_groups,       # NEW\n",
    "            dtype=None\n",
    "        ):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"  # NEW\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        ############################# NEW  #############################\n",
    "        # self.W_key = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        # self.W_value = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "        ################################################################\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None, cos=None, sin=None):\n",
    "        ##################### NEW  #####################\n",
    "        # The forward method now accepts `mask` instead of accessing it via self.mask.\n",
    "        # Also, we now have cos and sin as input for RoPE\n",
    "        ################################################    \n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        queries = self.W_query(x)  # Shape: (b, num_tokens, d_out)\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, num_kv_groups * head_dim)\n",
    "        values = self.W_value(x)  # Shape: (b, num_tokens, num_kv_groups * head_dim)\n",
    "\n",
    "        # Reshape queries, keys, and values\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        ##################### NEW  #####################\n",
    "        # keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        # values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "        ################################################\n",
    "\n",
    "        # Transpose keys, values, and queries\n",
    "        keys = keys.transpose(1, 2)  # Shape: (b, num_kv_groups, num_tokens, head_dim)\n",
    "        values = values.transpose(1, 2)  # Shape: (b, num_kv_groups, num_tokens, head_dim)\n",
    "        queries = queries.transpose(1, 2)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "\n",
    "        ##################### NEW #####################\n",
    "        # Apply RoPE\n",
    "        if cos is not None:\n",
    "            keys = compute_rope(keys, cos, sin)\n",
    "            queries = compute_rope(queries, cos, sin)\n",
    "        ################################################\n",
    "\n",
    "        ##################### NEW  #####################\n",
    "        # Expand keys and values to match the number of heads\n",
    "        # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        values = values.repeat_interleave(self.group_size, dim=1)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        # For example, before repeat_interleave along dim=1 (query groups):\n",
    "        #   [K1, K2]\n",
    "        # After repeat_interleave (each query group is repeated group_size times):\n",
    "        #   [K1, K1, K2, K2]\n",
    "        # If we used regular repeat instead of repeat_interleave, we'd get:\n",
    "        #   [K1, K2, K1, K2]\n",
    "        ################################################\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        # Shape: (b, num_heads, num_tokens, num_tokens)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        ##################### NEW #####################\n",
    "        # Create mask on the fly\n",
    "        if mask is None:\n",
    "            mask = torch.triu(torch.ones(num_tokens, num_tokens, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        ################################################\n",
    "    \n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        assert keys.shape[-1] == self.head_dim\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0a8d89",
   "metadata": {},
   "source": [
    "**LLama 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d170cd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_key: torch.Size([4096, 4096])\n",
      "W_value: torch.Size([4096, 4096])\n",
      "W_query: torch.Size([4096, 4096])\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "batch_size = 1\n",
    "context_len = 3000\n",
    "max_context_len = 8192\n",
    "embed_dim = 4096\n",
    "num_heads = 32\n",
    "\n",
    "\n",
    "example_batch = torch.randn((batch_size, context_len, embed_dim))\n",
    "\n",
    "mha = MultiHeadAttention(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    context_length=max_context_len,\n",
    "    num_heads=num_heads\n",
    ")\n",
    "\n",
    "mha(example_batch)\n",
    "\n",
    "print(\"W_key:\", mha.W_key.weight.shape)\n",
    "print(\"W_value:\", mha.W_value.weight.shape)\n",
    "print(\"W_query:\", mha.W_query.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be8020d",
   "metadata": {},
   "source": [
    "* Енді, егер біз grouped-query attention механизмын 8 kv-топқа бөлсек (Llama 3 8B дәл осылай жасайды), онда кілт (key) және мән (value) матрицаларындағы жолдар санының 4 есеге азайғанын байқаймыз (өйткені 32 attention басы 8 kv-топқа бөлінгенде 4 шығады).\n",
    "---\n",
    "* Теперь, если мы используем grouped-query attention с 8 kv-группами (именно столько используется в Llama 3 8B), можно увидеть, что количество строк в матрицах ключей (key) и значений (value) уменьшается в 4 раза (так как 32 головы внимания делятся на 8 kv-групп, получаем 4).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acd1156",
   "metadata": {},
   "source": [
    "**Llama 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1a577ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_key: torch.Size([1024, 4096])\n",
      "W_value: torch.Size([1024, 4096])\n",
      "W_query: torch.Size([4096, 4096])\n"
     ]
    }
   ],
   "source": [
    "gqa = GroupedQueryAttention(\n",
    "    d_in=embed_dim,\n",
    "    d_out=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_kv_groups=8,\n",
    ")\n",
    "\n",
    "gqa(example_batch)\n",
    "\n",
    "print(\"W_key:\", gqa.W_key.weight.shape)\n",
    "print(\"W_value:\", gqa.W_value.weight.shape)\n",
    "print(\"W_query:\", gqa.W_query.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24083a9a",
   "metadata": {},
   "source": [
    "* Қосымша ескерту ретінде: GroupedQueryAttention-ды стандартты көпбасты attention (multi-head attention) механизміне тең ету үшін, сұраныс топтарының санын (`num_kv_groups`) бастар санына (`num_heads`) тең етіп орнатуға болады.\n",
    "* Соңында, төменде параметрлер санын салыстырып көрейік:\n",
    "---\n",
    "* В качестве примечания: чтобы сделать GroupedQueryAttention эквивалентным стандартному механизму multi-head attention, можно установить количество групп запросов (`num_kv_groups`) равным количеству голов (`num_heads`).\n",
    "* Наконец, давайте сравним количество параметров ниже:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19a766d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:\n",
      "MHA: 67,108,864\n",
      "GQA: 41,943,040\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of parameters:\")\n",
    "\n",
    "mha_total_params = sum(p.numel() for p in mha.parameters())\n",
    "print(f\"MHA: {mha_total_params:,}\")\n",
    "\n",
    "gqa_total_params = sum(p.numel() for p in gqa.parameters())\n",
    "print(f\"GQA: {gqa_total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "114341f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory:\n",
    "del mha\n",
    "del gqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd538e7",
   "metadata": {},
   "source": [
    "## LLama 3 моделын жасау\n",
    "***\n",
    "## Создание Llama 3 модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6a60a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Llama2Model(nn.Module):\n",
    "class Llama3Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "        #################### NEW #####################\n",
    "        cos, sin = precompute_rope_params(\n",
    "            head_dim=cfg[\"emb_dim\"] // cfg[\"n_heads\"],\n",
    "            theta_base=cfg[\"rope_base\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            freq_config=cfg[\"rope_freq\"]\n",
    "        )\n",
    "        \n",
    "        self.register_buffer(\"cos\", cos, persistent=False)\n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "        ##############################################\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        x = tok_embeds\n",
    "\n",
    "        #################### NEW #####################\n",
    "        num_tokens = x.shape[1]\n",
    "        mask = torch.triu(torch.ones(num_tokens, num_tokens, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        ##############################################\n",
    "        \n",
    "        for block in self.trf_blocks:\n",
    "            x = block(x, mask, self.cos, self.sin)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "723311ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA3_CONFIG_8B = {\n",
    "    \"vocab_size\": 128_256,   # NEW: Larger vocabulary size\n",
    "    \"context_length\": 8192,  # NEW: Larger context length\n",
    "    \"emb_dim\": 4096,         # Embedding dimension\n",
    "    \"n_heads\": 32,           # Number of attention heads\n",
    "    \"n_layers\": 32,          # Number of layers\n",
    "    \"hidden_dim\": 14_336,    # NEW: Larger size of the intermediate dimension in FeedForward\n",
    "    \"n_kv_groups\": 8,        # NEW: Key-Value groups for grouped-query attention\n",
    "    \"rope_base\": 500_000.0,  # NEW: The base in RoPE's \"theta\" was increased to 500_000\n",
    "    \"rope_freq\": None,       # NEW: Additional configuration for adjusting the RoPE frequencies\n",
    "    \"dtype\": torch.bfloat16  # Lower-precision dtype to reduce memory usage\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cefa336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 8,835,567,616\n"
     ]
    }
   ],
   "source": [
    "model = Llama3Model(LLAMA3_CONFIG_8B)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afd341ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (PyTorch default): 74.09 GB\n",
      "bfloat16: 37.04 GB\n"
     ]
    }
   ],
   "source": [
    "def model_memory_size(model, input_dtype=torch.float32):\n",
    "    total_params = 0\n",
    "    total_grads = 0\n",
    "    for param in model.parameters():\n",
    "        # Calculate total number of elements per parameter\n",
    "        param_size = param.numel()\n",
    "        total_params += param_size\n",
    "        # Check if gradients are stored for this parameter\n",
    "        if param.requires_grad:\n",
    "            total_grads += param_size\n",
    "\n",
    "    # Calculate buffer size (non-parameters that require memory)\n",
    "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
    "\n",
    "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
    "    # We assume parameters and gradients are stored in the same type as input dtype\n",
    "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
    "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
    "\n",
    "    # Convert bytes to gigabytes\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    return total_memory_gb\n",
    "\n",
    "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
    "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e2e6e",
   "metadata": {},
   "source": [
    "# Llama 3.1 8B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4768aab4",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/llama3-to-llama31.webp\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2b0500",
   "metadata": {},
   "source": [
    "* Архитектура бірдей, тек төмендегі конфигурация файлында көрсетілгендей RoPE жиіліктерін қайта масштабтау ғана өзгеше.\n",
    "---\n",
    "* Архитектура идентична, единственное изменение — это перемасштабирование частот RoPE, как указано в конфигурационном файле ниже.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0dd1de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA31_CONFIG_8B = {\n",
    "    \"vocab_size\": 128_256,      # Vocabulary size\n",
    "    \"context_length\": 131_072,  # NEW: Larger supported context length\n",
    "    \"emb_dim\": 4096,            # Embedding dimension\n",
    "    \"n_heads\": 32,              # Number of attention heads\n",
    "    \"n_layers\": 32,             # Number of layers\n",
    "    \"hidden_dim\": 14_336,       # Size of the intermediate dimension in FeedForward\n",
    "    \"n_kv_groups\": 8,           # Key-Value groups for grouped-query attention\n",
    "    \"rope_base\": 500_000.0,     # The base in RoPE's \"theta\"\n",
    "    \"dtype\": torch.bfloat16,    # Lower-precision dtype to reduce memory usage\n",
    "    \"rope_freq\": {              # NEW: RoPE frequency scaling\n",
    "        \"factor\": 8.0,\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"original_context_length\": 8192,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b733aa",
   "metadata": {},
   "source": [
    "* Бұрын кодта көргеніміздей, RoPE әдісі позициялық ақпаратты тікелей attention механизміне енгізу үшін синусоидалы функцияларды (синус және косинус) пайдаланады.\n",
    "* Llama 3.1-де қосымша конфигурация арқылы кері жиілік (inverse frequency) есептеулеріне қосымша түзетулер енгізіледі.\n",
    "* Бұл түзетулер әртүрлі жиілік компоненттерінің позициялық ендірулерге (positional embeddings) қалай әсер ететінін өзгертеді (бұл тақырыпты кейінірек толығырақ талқылаймыз).\n",
    "---\n",
    "* Как мы уже видели в коде ранее, метод RoPE использует синусоидальные функции (синус и косинус), чтобы встроить позиционную информацию напрямую в механизм внимания.\n",
    "* В Llama 3.1, благодаря дополнительной конфигурации, вводятся дополнительные корректировки в расчёт обратных частот (inverse frequency).\n",
    "* Эти корректировки влияют на то, как разные частотные компоненты вносят вклад в позиционные эмбеддинги (подробное объяснение этого оставим на потом)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe206259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "# free up memory\n",
    "del model\n",
    "\n",
    "gc.collect()  # Run Python garbage collector\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f15da14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Thin wrapper around tiktoken that keeps track of Llama-3 special IDs.\"\"\"\n",
    "    def __init__(self, model_path):\n",
    "        if not os.path.isfile(model_path):\n",
    "            raise FileNotFoundError(model_path)\n",
    "\n",
    "        mergeable = load_tiktoken_bpe(model_path)\n",
    "\n",
    "        # hard-coded from Meta's tokenizer.json\n",
    "        self.special = {\n",
    "            \"<|begin_of_text|>\": 128000,\n",
    "            \"<|end_of_text|>\": 128001,\n",
    "            \"<|start_header_id|>\": 128006,\n",
    "            \"<|end_header_id|>\": 128007,\n",
    "            \"<|eot_id|>\": 128009,\n",
    "        }\n",
    "        self.special.update({f\"<|reserved_{i}|>\": 128002 + i\n",
    "                             for i in range(256)\n",
    "                             if 128002 + i not in self.special.values()})\n",
    "\n",
    "        self.model = tiktoken.Encoding(\n",
    "            name=Path(model_path).name,\n",
    "            pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)\"\n",
    "                    r\"|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+\"\n",
    "                    r\"|\\p{N}{1,3}\"\n",
    "                    r\"| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*\"\n",
    "                    r\"|\\s*[\\r\\n]+\"\n",
    "                    r\"|\\s+(?!\\S)\"\n",
    "                    r\"|\\s+\",\n",
    "            mergeable_ranks=mergeable,\n",
    "            special_tokens=self.special,\n",
    "        )\n",
    "\n",
    "    def encode(self, text, bos=False, eos=False):\n",
    "        ids = ([self.special[\"<|begin_of_text|>\"]] if bos else []) \\\n",
    "              + self.model.encode(text)\n",
    "        if eos:\n",
    "            ids.append(self.special[\"<|end_of_text|>\"])\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return self.model.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8c4ad0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "login(token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d63ee5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1d78ff8bb14f3e8fabd70b42399466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "original/tokenizer.model:   0%|          | 0.00/2.18M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "tokenizer_file_path = hf_hub_download(\n",
    "    repo_id=\"meta-llama/Llama-3.1-8B\",\n",
    "    filename=\"original/tokenizer.model\",\n",
    "    local_dir=\"Llama-3.1-8B\"\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(tokenizer_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d0ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Llama3Model(LLAMA31_CONFIG_8B)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01bf6ba",
   "metadata": {},
   "source": [
    "# Llama 3.2 1B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e839c835",
   "metadata": {},
   "source": [
    "\n",
    "* Llama 3.2  моделінің коды Llama 3.1-ге ұқсас, тек модельдің өлшемі кішірейген (1B және 3B нұсқалары бар).\n",
    "* Тағы бір тиімділігі — салмақтарды байланыстыруды (weight tying) қайта қосу болды (бұл идея алғаш рет GPT-2 архитектурасында қолданылған); мұнда кіріс (token embedding) қабаты мен шығыс қабатында бірдей салмақ параметрлері қайта пайдаланылады.\n",
    "* Llama 3.2 1B моделі өте ықшам, себебі ол тіпті көптеген мобильді құрылғыларда да жұмыс істей алады.\n",
    "* Llama 3.1 8B және Llama 3.2 1B модельдерінің архитектуралық айырмашылықтары төмендегі суретте көрсетілген.\n",
    "---\n",
    "* Код  модели Llama 3.2 похож на Llama 3.1, за исключением того, что размер модели уменьшился (существуют версии на 1B и 3B параметров).\n",
    "* Ещё одно улучшение эффективности заключается в возвращении приёма «weight tying» (связывания весов) — концепции, впервые использованной в архитектуре GPT-2; здесь те же значения весовых параметров используются как в входном (token embedding) слое, так и в выходном.\n",
    "* Малый размер модели Llama 3.2 1B делает её весьма удобной, поскольку она может работать даже на многих мобильных устройствах.\n",
    "* Архитектурные различия между Llama 3.1 8B и Llama 3.2 1B показаны на рисунке ниже.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccfa593",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/llama31-to-llama32.webp?1\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb55a9",
   "metadata": {},
   "source": [
    "* Жоғарыдағы суретке қарағанда, Llama 3.1 8B мен Llama 3.2 1B архитектураларының негізгі айырмашылығы — олардың өлшемдерінде.\n",
    "* Қосымша шағын өзгеріс — RoPE қайта масштабтау коэффициентінің ұлғаюы, бұл төмендегі конфигурация файлында көрсетілген.\n",
    "---\n",
    "* Как видно из приведённого выше рисунка, основное различие между архитектурами Llama 3.1 8B и Llama 3.2 1B заключается в их размерах.\n",
    "* Небольшое дополнительное изменение — увеличение коэффициента рескейлинга (перемасштабирования) RoPE, что отражено в конфигурационном файле ниже.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dde16b5",
   "metadata": {},
   "source": [
    "* Төменде біз Llama 3.1 8B бөліміндегі кодты қайта қолданып, Llama 3.2 1B моделін жүктей аламыз.\n",
    "* Тағы да ескерту ретінде: Llama 3.2 моделі Llama 3.1 отбасынан бөлек болғандықтан, [meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B) репозиторийіне кіріп, лицензиялық шарттармен келісу қажет — әйтпесе Hugging Face access token арқылы жүктеу жұмыс істемейді.\n",
    "* Кеңес: Қарапайымдылық үшін төменде тек базалық модель жүктеледі, бірақ `\"meta-llama/Llama-3.2-1B\"` атауын `\"meta-llama/Llama-3.2-1B-Instruct\"` деп ауыстыру арқылы нұсқаулықпен (instruction) бапталған нұсқасын да қолдануға болады.\n",
    "---\n",
    "* Ниже мы можем повторно использовать код из раздела Llama 3.1 8B, чтобы загрузить модель Llama 3.2 1B.\n",
    "* Опять же, поскольку семейство Llama 3.2 отличается от Llama 3.1, необходимо перейти в репозиторий [meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B) и принять условия лицензии, чтобы ваш токен доступа Hugging Face работал при загрузке.\n",
    "* Совет: Для простоты ниже мы загружаем только базовую модель, но вы также можете использовать инструкционно дообученную версию, заменив `\"meta-llama/Llama-3.2-1B\"` на `\"meta-llama/Llama-3.2-1B-Instruct\"`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f64c1e",
   "metadata": {},
   "source": [
    "# Testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6e4c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from google.colab import userdata\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=userdata.get('HF_TOKEN'))\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"mps\",\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd9fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs[0][\"generated_text\"][-1]['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
