{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19929af",
   "metadata": {},
   "source": [
    "# Тарау 1: Мәтіндерді өңдеу\n",
    "# Раздел 1: Обработка текстовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17946c7",
   "metadata": {},
   "source": [
    "1. Бұл блокнотта қолданылатын пакеттер:\n",
    "2. Библиотеки, которые используются в этом блокноте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86cde18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0a0f4a",
   "metadata": {},
   "source": [
    "- Бұл тарауда  деректерді өңдеу және бөлшектерге бөлу қарастырылады\n",
    "- В этой главе мы рассмотрим обработку текстовых данных и разделение их на части."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0889af3b",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/01.webp?timestamp=1\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416bc000",
   "metadata": {},
   "source": [
    "## 2.1 Cөз эмбедингтерін (векторлық көріністер) түсіндіру\n",
    "## 2.1 Понимание эмбеддингов (векторные представления) слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1c92b1",
   "metadata": {},
   "source": [
    "Эмбеддингтердің көптеген түрлері бар; бұл жұмыста біз мәтіндік эмбеддингтерге назар аударамыз.\n",
    "\n",
    "Существует множество видов эмбеддингов; в этой работе мы сосредоточимся на текстовых эмбеддингах.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608d9190",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/02.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ef2894",
   "metadata": {},
   "source": [
    "- Ауқымды Тіл Модельдері (ATM) жоғары өлшемді кеңістіктердегі (яғни, мыңдаған өлшемдері бар) эмбеддингтермен жұмыс істейді.\n",
    "- Языковые модели работают с эмбеддингами в высокоразмерных пространствах (т. е. с тысячами измерений).\n",
    "***\n",
    "- Біз мұндай жоғары өлшемді кеңістіктерді көрнекі түрде елестете алмайтындықтан (өйткені адамдар 1, 2 немесе 3 өлшеммен ойлайды),     \n",
    "төмендегі суретте 2 өлшемді эмбеддинг кеңістігі көрсетілген.\n",
    "- Поскольку мы не можем визуализировать такие многомерные пространства (ведь люди мыслят в 1, 2 или 3 измерениях),    \n",
    "на рисунке ниже показано двумерное пространство эмбеддингов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd6279",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/03.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eff7c1",
   "metadata": {},
   "source": [
    "## 2.2 Текст токендеу\n",
    "## 2.2 Токенизация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4bda65",
   "metadata": {},
   "source": [
    "- Бұл бөлімде біз мәтінді токендейміз, яғни оны жеке сөздер мен тыныс белгілері сияқты кішірек бірліктерге бөлеміз.\n",
    "- В этом разделе мы токенизируем текст, то есть разделяем его на более мелкие единицы, такие как отдельные слова и знаки препинания."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfb4241",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/04.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10e9049",
   "metadata": {},
   "source": [
    "- Бастапқы мәтін ретінде Герберт Уэлс жазушының \"Time Machine\" шығармасын қолданамыз.\n",
    "- В качестве исходного текста мы используем произведение Герберта Уэлса \"Time Machine\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3046bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"time_machine.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/Azamat0315277/LLM_from_scratch/refs/heads/main/ch01/time_machine.txt\")\n",
    "    file_path = \"time_machine.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"time_machine.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[20:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f2704b",
   "metadata": {},
   "source": [
    "- Мақсат — осы мәтінді АТМ үшін токендеу және одан эмбединг жасау.  \n",
    "- Кейінірек жоғарыдағы мәтінге қолдану үшін, қарапайым үлгі мәтін негізінде қарапайым токендегіш әзірлейміз.   \n",
    "- Төмендегі тұрақты өрнек (Regular Expression)  сөздерді бос орындар бойынша бөледі.    \n",
    "\n",
    " <br>\n",
    " \n",
    "- Цель — токенизировать текст и создать эмбеддинги для LLM.  \n",
    "- Давайте разработаем простой токенизатор на основе простого примера текста, который мы затем сможем применить к тексту выше.\n",
    "- Следующее регулярное выражение (Regular Expression) будет выполнять разделение по пробельным символам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c502a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d7fa6",
   "metadata": {},
   "source": [
    "- Бізге мәтінді тек бос орындар бойынша ғана емес, үтірлер мен нүктелер бойынша да бөлу керек, сондықтан осыған сәйкес тұрақты өрнекті өзгертейік.\n",
    "- Нам необходимо выполнять разделение не только по пробельным символам, но и по запятым и точкам, поэтому давайте изменим регулярное выражение чтобы оно учитывало и это."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310f344c",
   "metadata": {},
   "source": [
    "- Көріп тұрғанымыздай, бұл сөз арасындағы бос орындарды көбейтті, сондықтан оларды да алып тастайық.\n",
    "- Как мы видим, это создаёт пустые строки — давайте и их удалим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3841fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip whitespace from each item and then filter out any empty strings.\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5872483c",
   "metadata": {},
   "source": [
    "- Бұл әжептәуір жақсы, бірақ нүктелер, сұрақ белгілері және т.б. сияқты тыныс белгілерінің басқа түрлерін де ескерейік.\n",
    "- Выглядит неплохо, но давайте также обработаем и другие знаки препинания, такие как точки, вопросительные знаки и так далее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a53f894",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, world—. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()—\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e2d034",
   "metadata": {},
   "source": [
    "- Нәтиже жақсы, енді біз осы токендеу әдісін бастапқы мәтінге қолдануға дайынбыз.\n",
    "- Очень неплохо, и теперь мы готовы применить эту токенизацию к исходному тексту."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198e28ed",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/05.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2160eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()—*’“$%\\']|--|——|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db016d52",
   "metadata": {},
   "source": [
    "- Токендердің жалпы санын есептейік.\n",
    "- Давайте посчитаем общее количество токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b121e3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9250b19",
   "metadata": {},
   "source": [
    "## 2.3 Токендерді токен ID-ларына айналдыру\n",
    "## 2.3 Преобразование токенов в идентификаторы (ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1241c1",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/06.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c02b7ae",
   "metadata": {},
   "source": [
    "- Енді осы токендерден барлық бірегей токендерді қамтитын сөздік құра аламыз.\n",
    "- Теперь из этих токенов можно составить словарь, состоящий из всех уникальных токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8d3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3052e13",
   "metadata": {},
   "source": [
    "- Сөздік құру\n",
    "- Составляем словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dcbc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20437bb1",
   "metadata": {},
   "source": [
    "- Сөздіктегі алғашқы 50 сөзді көрсету\n",
    "- Ниже приведены первые 50 элементов этого словаря:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceadeeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 100:\n",
    "       break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df69bfb",
   "metadata": {},
   "source": [
    "## Link:\n",
    "* Tokenizer visualizer: https://tiktokenizer.vercel.app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c6e56",
   "metadata": {},
   "source": [
    "- Төменде шағын сөздікті пайдаланып, қысқа үлгі мәтінді токендеу үдерісі көрсетілген:\n",
    "- Ниже показана токенизация небольшого образца текста с использованием маленького словаря:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe3fb0c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/07.webp?123\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdac41d",
   "metadata": {},
   "source": [
    "- Енді барлығын токендеу `Класына` біріктіреміз:\n",
    "- Теперь соберём всё вместе в `Kласс` токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d98f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        # Preprocess the text\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()—*’“$%\\']|--|——|\\s)', text)\n",
    "        # Remove empty strings\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        # Convert strings to IDs\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()—*’“$%\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e2f5f",
   "metadata": {},
   "source": [
    "- `encode` функциясы **мәтінді** **токен ID-ларына** айналдырады.\n",
    "- `decode` функциясы **токен ID-ларын** **кері мәтінге** айналдырады.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Функция `encode` преобразует **текст** в **идентификаторы токенов (ID)**.\n",
    "- Функция `decode` преобразует **идентификаторы токенов (ID)** обратно в **текст**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b604cc",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/08.webp?123\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2e9e35",
   "metadata": {},
   "source": [
    "- Біз токендеу құралын пайдаланып, **мәтіндерді** **бүтін сандарға** кодтай (яғни, токендей) аламыз.\n",
    "- Содан кейін бұл **бүтін сандарды** АТМ үшін кіріс деректері ретінде **эмбеддинг жасауға** болады."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc827f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"Well, I do not mind telling you I have been at work upon this geometry\n",
    "          of Four Dimensions for some time\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4e6318",
   "metadata": {},
   "source": [
    "- Бүтін сандарды кері мәтінге декодтауға болады. \n",
    "- Мы можем декодировать целые числа обратно в текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da084ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ada69a",
   "metadata": {},
   "source": [
    "## 2.4 **Арнайы контекст** бар токендерін қосу \n",
    "## 2.4 Добавление **специальных контекстных** токенов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da8b0ad",
   "metadata": {},
   "source": [
    "- **Бейтаныс сөздерді** және **мәтіннің соңын** белгілеу үшін бірнеше «арнайы» токендер қосқан пайдалы.\n",
    "- Полезно добавить несколько «специальных» токенов для обозначения **неизвестных слов** и **конца текста**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96928ed7",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/09.webp?123\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996e9a3a",
   "metadata": {},
   "source": [
    "- Кейбір токенизаторлар АТМ-ге қосымша контекст беру үшін арнайы токендерді пайдаланады.  \n",
    "Осындай арнайы токендердің кейбірі:  \n",
    "    - [BOS] (тізбектің басы) мәтіннің басын белгілейді.\n",
    "    - [EOS] (тізбектің соңы) мәтіннің қай жерде аяқталатынын белгілейді (бұл әдетте бір-бірімен байланыссыз бірнеше мәтінді, мысалы, екі түрлі Уикипедия мақаласын немесе екі түрлі кітапты біріктіру үшін қолданылады).\n",
    "    - [PAD] (толтыру) егер АТМ-ді 1-ден үлкен топтама өлшемімен оқытсақ қолданылады (біз әртүрлі ұзындықтағы бірнеше мәтінді қосуымыз мүмкін; толтыру токенімен барлық мәтіндердің ұзындығы бірдей болуы үшін қысқа мәтіндерді ең ұзынына дейін толтырамыз).\n",
    "    - [UNK] сөздікке кірмеген сөздерді белгілеу үшін қолданылады.\n",
    "- Айта кететін жайт, `GPT-2` жоғарыда аталған токендердің ешқайсысын қажет етпейді, бірақ күрделілікті азайту үшін тек  `<|endoftext|>` токенін пайдаланады.\n",
    "`<|endoftext|>` жоғарыда аталған [EOS] токеніне ұқсас.  \n",
    "- GPT сондай-ақ `<|endoftext|>` токенін толтыру үшін де пайдаланады (себебі біз топтамалық енгізулермен оқыту кезінде әдетте масканы қолданамыз, сондықтан толтырылған токендерге назар аудармаймыз, сәйкесінше ол токендердің қандай болғаны маңызды емес).\n",
    "- GPT-2 сөздіктен тыс сөздер үшін <UNK> токенін пайдаланбайды; оның орнына GPT-2 байт жұбымен кодтау (BPE) токенизаторын қолданады, ол сөздерді біз кейінгі бөлімде талқылайтын сөз бөліктеріне бөледі."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f26b1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Некоторые токенизаторы используют специальные токены, чтобы предоставить LLM дополнительный контекст.   \n",
    "Вот некоторые из этих специальных токенов:\n",
    "    - [BOS] (начало последовательности) отмечает начало текста.\n",
    "    - [EOS] (конец последовательности) отмечает место окончания текста (обычно используется для объединения нескольких несвязанных текстов, например, двух разных статей из Википедии или двух разных книг и т. д.).\n",
    "    - [PAD] (заполнение/паддинг) используется, если мы обучаем LLM с размером батча больше 1 (мы можем включать несколько текстов разной длины; с помощью токена-заполнителя мы дополняем более короткие тексты до наибольшей длины, чтобы все тексты имели одинаковую длину).\n",
    "    - [UNK] используется для представления слов, которые не включены в словарь.\n",
    "\n",
    "- Обратите внимание, что `GPT-2` не требует ни одного из упомянутых выше токенов, а использует только токен `<|endoftext|>` для уменьшения сложности.  \n",
    "`<|endoftext|>` аналогичен упомянутому выше токену [EOS].\n",
    "- GPT также использует `<|endoftext|>` для паддинга (поскольку при обучении на пакетных данных мы обычно используем маску, мы всё равно не будем обращать внимание на токены-заполнители, поэтому не имеет значения, что это за токены).\n",
    "- GPT-2 не использует токен <UNK> для слов, отсутствующих в словаре; вместо этого GPT-2 использует токенизатор на основе попарного кодирования байтов (BPE), который разбивает слова на подслова (суб-единицы), что мы обсудим в следующем разделе."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0873ffd4",
   "metadata": {},
   "source": [
    "- Біз `<|endoftext|>` токендерін екі тәуелсіз мәтін көзінің арасында қолданамыз:\n",
    "- Мы используем токены `<|endoftext|>` между двумя независимыми источниками текста:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca3796",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/10.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a884e662",
   "metadata": {},
   "source": [
    "- Төмендегі мәтінді `токендегенде` не болатынын көрейік:\n",
    "- Давайте посмотрим, что произойдёт, если мы `токенизируем` следующий текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bc05dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"Hello, do you like tea. Is this-- a test?\"\n",
    "\n",
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4110a950",
   "metadata": {},
   "source": [
    "- Жоғарыдағы код қате шығарады, себебі `«Hello»` сөзі сөздікте жоқ.  \n",
    "- Мұндай жағдайларды шешу үшін сөздікке бейтаныс сөздерді белгілейтін `<|unk|>` сияқты арнайы токендерді қоса аламыз.\n",
    "- Сөздікті кеңейтіп жатқандықтан, GPT-2 оқытуында мәтіннің соңын белгілеу үшін қолданылатын `<|endoftext|>` деп аталатын тағы бір токен қосайық (ол сондай-ақ біріктірілген мәтіндер арасында да қолданылады, мысалы, егер оқыту деректер жиынымыз бірнеше мақаладан, кітаптан және т.б. тұрса)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00afa7dd",
   "metadata": {},
   "source": [
    "- Приведенный выше код выдаёт ошибку, потому что слово `«Hello»` отсутствует в словаре.\n",
    "- Чтобы справиться с такими случаями, мы можем добавить в словарь специальные токены, такие как `<|unk|>`, для представления неизвестных слов.\n",
    "- Поскольку мы уже расширяем словарь, давайте добавим ещё один токен, `<|endoftext|>`,  \n",
    "который используется при обучении GPT-2 для обозначения конца текста (он также используется между объединёнными текстами,    \n",
    "например, если наши обучающие датасеты состоят из нескольких статей, книг и т. д.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f75731",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84964004",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fe4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9921e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        # Preprocess the text\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()—*’“$%\\']|--|——|\\s)', text)\n",
    "        # Remove empty strings\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        # Replace unknown tokens\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        # Convert tokens to IDs\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        # Convert IDs back to tokens\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()—*’“$%\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279ccb0b",
   "metadata": {},
   "source": [
    "- Өзгертілген токендеу құралымен мәтінді токендеп көрейік:\n",
    "- Давайте попробуем токенизировать текст с помощью изменённого токенизатора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9afa317",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e2e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197d1a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a665eb2c",
   "metadata": {},
   "source": [
    "## 2.5 BytePair кодтау (Байт жұбымен кодтау)\n",
    "## 2.5 BytePair кодирование (Попарное кодирование байтов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6249b4a4",
   "metadata": {},
   "source": [
    "- GPT-2 өз токендегіші ретінде BytePair (Байт жұбымен кодтауды) (BPE) пайдаланды.\n",
    "- Бұл модельге алдын ала анықталған сөздікте жоқ сөздерді кішірек сөз бөліктеріне немесе тіпті жеке таңбаларға бөлуге мүмкіндік береді, осылайша ол сөздіктен тыс сөздерді өңдей алады.\n",
    "- Мысалы, егер GPT-2 сөздігінде `«unfamiliarword»` деген сөз болмаса, ол оны өзінің үйретілген BPE біріктірулеріне байланысты `[\"unfam\", \"iliar\", \"word\"]` немесе басқа сөз бөліктеріне бөліп токендеуі мүмкін.   \n",
    "**Мысалы:**    \n",
    "`Walker walked a long way` -> `[w, a, l, k, e, r, d, o, n, g, y, al, alk, alke, walke]`\n",
    "***\n",
    "- Түпнұсқа BPE токендегішін мына жерден табуға болады: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
    "- Бұл тарауда біз OpenAI-дың ашық бастапқы кодты `tiktoken` пакеттың `BPE` токендегішін қолданамыз. Бұл пакеттың негізгі алгоритмдері есептеу өнімділігін жақсарту үшін Rust тілінде жазылған."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b8995",
   "metadata": {},
   "source": [
    "- GPT-2 использовал попарное кодирование байтов (BPE) в качестве своего токенизатора.\n",
    "- Это позволяет модели разбивать слова, которых нет в её предопределённом словаре, на более мелкие подслова или даже отдельные символы, что даёт возможность обрабатывать слова, отсутствующие в словаре.\n",
    "- Например, если в словаре GPT-2 нет слова «unfamiliarword», он может токенизировать его как [\"unfam\", \"iliar\", \"word\"] или разбить на другие подслова, в зависимости от своих обученных слияний BPE.    \n",
    "**Пример:**    \n",
    "`Walker walked a long way` -> `[w, a, l, k, e, r, d, o, n, g, y, al, alk, alke, walke]`  \n",
    "***\n",
    "- Оригинальный токенизатор BPE можно найти здесь: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
    "- В этой главе мы используем токенизатор BPE из библиотеки с открытым исходным кодом tiktoken от OpenAI, которая реализует свои основные алгоритмы на Rust для повышения вычислительной производительности."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda6a67",
   "metadata": {},
   "source": [
    "## LINK\n",
    "* BPE visualizer: https://www.bpe-visualizer.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cea2053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ca6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f077e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defbf4df",
   "metadata": {},
   "source": [
    "- BPE токендегіштері бейтаныс сөздерді сөз бөліктеріне және жеке таңбаларға бөледі:\n",
    "- BPE-токенизаторы разбивают неизвестные слова на подслова и отдельные символы:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b595c654",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/11.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559fd044",
   "metadata": {},
   "source": [
    "## 2.6 Жылжымалы терезе әдісі арқылы деректерді іріктеу\n",
    "## 2.6 Выборка данных методом скользящего окна"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3150eb13",
   "metadata": {},
   "source": [
    "- Біз АТМ-дерді  бір сөз генерациялауға үйрететіндіктен, оқыту деректерде келесі сөз  ағымдағы сөзге болжау үшін нысана болатындай етіп дайындаймыз:\n",
    "- Мы обучаем LLM генерировать по одному слову за раз, поэтому обучающие данные нужно подготовить так, чтобы следующее слово в последовательности было целью для предсказания :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd0d05e",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/12.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d34d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"time_machine.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "# Encoding text using new tokenizer\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45327306",
   "metadata": {},
   "source": [
    "- Әрбір мәтін бөлігі үшін бізге кіріс деректер мен нысана деректер қажет.\n",
    "- Модельдің келесі сөзді болжағанын қалайтындықтан, нысаналар дегеніміз — оңға қарай бір позицияға ығыстырылған кіріс деректер.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Для каждого фрагмента текста нам нужны входящие данные и целевые данные.  \n",
    "- Поскольку мы хотим, чтобы модель предсказывала следующее слово, цели — это входы, сдвинутые на одну позицию вправо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7aa336",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b6120",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff270892",
   "metadata": {},
   "source": [
    "- Болжау әдісі төмендегідей болады:\n",
    "- Шаг за шагом предсказание будет выглядеть следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38dd880",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ee4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9ab12f",
   "metadata": {},
   "source": [
    "- Келесі сөзді болжауды біз `зейін механизмін` (attention mechanism) қарастырып болғаннан кейін, кейінгі тарауда талқылаймыз.\n",
    "- Әзірше біз кіріс деректер жиынын топтастап, бір позицияға ығыстырылған кіріс деректер мен нысана деректерді қайтаратын қарапайым деректер жүктегішін жүзеге асырамыз.\n",
    "\n",
    "- Мы займёмся предсказанием следующего слова в одной из последующих глав, после того как изучим механизм внимания (attention mechanism).\n",
    "- А пока мы реализуем простой загрузчик данных (dataloader), который проходит по входному набору данных и возвращает входы и цели, сдвинутые на одну позицию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77493be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f1b0e7",
   "metadata": {},
   "source": [
    "- Позицияны +1 сөзге өзгерте отырып, жылжымалы терезе әдісін қолданамыз:\n",
    "- Мы используем подход скользящего окна, смещая позицию на +1 слово:\n",
    "\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/13.webp?123\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384a92aa",
   "metadata": {},
   "source": [
    "- Кіріс мәтіндік деректер жиынынан бөліктерді алатын деректер жиынын (dataset) және деректер жүктегішін (dataloader) құру.\n",
    "- Создание набора данных (dataset) и загрузчика данных (dataloader), которые извлекают фрагменты из входного текстового набора данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20441fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            # The input is a chunk of text of size 'max_length'\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            # The target is the same chunk, but shifted one position to the right.\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "\n",
    "            # Convert the chunks to PyTorch tensors and store them.\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    # Returns the total number of samples in the dataset.\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "        \n",
    "    # Retrieves one sample (an input and its corresponding target) from the dataset.\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc95d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941bd30e",
   "metadata": {},
   "source": [
    "- Контекст өлшемі (сөздер саны) 4 болатын АТМ үшін топтама өлшемі 1-ге тең деректер жүктегішін (dataloader) тексеріп көрейік:\n",
    "- Давайте протестируем загрузчик данных (dataloder) с размером батча 1 для LLM с размером контекста 4:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25db9599",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"time_machine.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# Create a dataloader instance using our custom function.\n",
    "dataloader = create_dataloader_v1(\n",
    "                raw_text,           # The source text to be processed into chunks.\n",
    "                batch_size=1,       # The number of text chunks in each batch.\n",
    "                max_length=4,       # The fixed length of each text chunk (in tokens).\n",
    "                stride=1,           # The step size to slide the window for creating chunks.\n",
    "                shuffle=False       # Whether to shuffle the order of the chunks (False means keep order).\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc545125",
   "metadata": {},
   "source": [
    "- Төменде қадамы контекст ұзындығына (бұл жерде: 4) тең болатын мысал көрсетілген:\n",
    "- Ниже приведён пример, в котором шаг равен длине контекста (здесь: 4):\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/14.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef381d71",
   "metadata": {},
   "source": [
    "- Біз сондай-ақ  нәтижелер деректерін  топтамамен (batch) де шығара аламыз.\n",
    "- Ескеріңіз, біз бұл жерде топтамалар арасында қиылысулар болмауы үшін қадамды ұлғайтамыз, себебі қиылысудың көп болуы шамадан тыс үйретудің артуына әкелуі мүмкін.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Мы также можем создавать пакетные (batch) выходные данные.\n",
    "- Обратите внимание, что здесь мы увеличиваем шаг, чтобы не было пересечений между пакетами, поскольку большее пересечение может привести к усилению переобучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17be362",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "                raw_text, \n",
    "                batch_size=8, \n",
    "                max_length=4, \n",
    "                stride=4, \n",
    "                shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4dc230",
   "metadata": {},
   "source": [
    "## 2.7 Токен эмбеддингтерін құру\n",
    "## 2.7 Создание эмбеддинги токенов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1446ad",
   "metadata": {},
   "source": [
    "- Деректер LLM (АТМ) үйрету үшін дайын деуге болады.\n",
    "- Соңында, эмбеддинг қабатын пайдаланып, токендерді үздіксіз векторлық көрініске (vectors) эмбеддейік.\n",
    "- Әдетте, бұл эмбеддинг қабаттары LLM-нің өз құрамына кіреді және модельді оқыту барысында жаңартылып (оқытылып) отырады.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Данные уже почти готовы для обучения LLM.\n",
    "- И наконец, давайте преобразуем токены в непрерывное векторное представление с помощью слоя эмбеддингов.\n",
    "- Обычно эти слои эмбеддингов являются частью самой LLM и обновляются (обучаются) в процессе обучения модели.\n",
    "\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/15.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d05d07",
   "metadata": {},
   "source": [
    "- Бізде (токендеуден кейін) кіріс ID-лары 2, 3, 5 және 1 болатын келесі төрт кіріс мысалы бар деп есептейік.\n",
    "- Предположим, у нас есть следующие четыре входных примера с ID 2, 3, 5 и 1 (после токенизации)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70feeb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b41be72",
   "metadata": {},
   "source": [
    "- Қарапайымдылық үшін, бізде бар болғаны 6 сөзден тұратын шағын сөздік бар және біз өлшемі 3-ке тең эмбеддингтер жасағымыз келеді деп есептейік:\n",
    "- Для простоты предположим, что у нас есть небольшой словарь всего из 6 слов, и мы хотим создать эмбеддинги размером 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5d64b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e6e303",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247b91e2",
   "metadata": {},
   "source": [
    "- ID-і 3-ке тең токенді 3-өлшемді векторға түрлендіру үшін келесіні орындаймыз:\n",
    "- Чтобы преобразовать токен с ID 3 в 3-мерный вектор, выполним следующее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305ac89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer(torch.tensor([0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a09883",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d78b7",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/16.webp?123\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2cf38",
   "metadata": {},
   "source": [
    "## 2.8 Сөз позицияларын кодтау\n",
    "## 2.8 Кодирование позиций слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a945b4",
   "metadata": {},
   "source": [
    "- Эмбеддинг қабаты токендерды деректерде \n",
    "орналасу орнын есепке алмай векторлық көріністерге айналдыра береді:\n",
    "- Слой эмбеддингов преобразует токены в идентичные векторные представления независимо от их расположения во входной последовательности:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80091c95",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/17.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d50a6",
   "metadata": {},
   "source": [
    "- **Позициялық эмбеддингтер** **ауқымды тілдік модель** үшін **кіріс эмбеддингтерін** құру мақсатында **токен эмбеддинг векторымен** біріктіріледі:\n",
    "\n",
    "-   **Позиционные эмбеддинги** объединяются с **вектором эмбеддинга токена**, чтобы сформировать **входные эмбеддинги** для **большой языковой модели**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f03195",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/18.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc185fca",
   "metadata": {},
   "source": [
    "-   **Байт жұбымен кодтаушының (BytePair encoder)** **сөздік қорының өлшемі 50 257-ге тең**:\n",
    "-   **Кіріс токендерін** **256-өлшемді векторлық көрініске** кодтағымыз келеді деп есептейік:\n",
    "\n",
    "<br>\n",
    "\n",
    "-   **Кодировщик BytePair** имеет **размер словаря 50 257**:\n",
    "-   Предположим, мы хотим закодировать **входные токены** в **256-мерное векторное представление**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266f0192",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac809748",
   "metadata": {},
   "source": [
    "- Деректер жүктегішінен (dataloader) деректерді іріктесек, біз әрбір топтамадағы токендерді 256-өлшемді векторға эмбеддейміз.\n",
    "- Егер бізде әрқайсысында 4 токені бар топтама (batch) өлшемі 8 болса, нәтижесінде 8 x 4 x 256 тензоры пайда болады:\n",
    "\n",
    "<br>\n",
    "\n",
    "- Если мы делаем выборку данных из загрузчика (dataloader), мы преобразуем токены в каждом пакете в 256-мерный вектор.\n",
    "- Если у нас размер пакета (батча) равен 8 и в каждом по 4 токена, в результате мы получим тензор размером 8 x 4 x 256:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "                raw_text, \n",
    "                batch_size=8, \n",
    "                max_length=max_length,\n",
    "                stride=max_length, \n",
    "                shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be579628",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a0bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "# First group of words\n",
    "# print(token_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e420dfe",
   "metadata": {},
   "source": [
    "- GPT-2 абсолютті позициялық эмбеддингтерді пайдаланады, \n",
    "сондықтан біз жай ғана тағы бір эмбеддинг қабатын құрамыз:\n",
    "- GPT-2 использует абсолютные позиционные эмбеддинги, \n",
    "поэтому мы просто создаём ещё один слой эмбеддингов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b65aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "# print(pos_embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3922b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "print(pos_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e4e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e51698b",
   "metadata": {},
   "source": [
    "- LLM-де қолданылатын кіріс эмбеддингтерін құру үшін біз токендік және позициялық эмбеддингтерді жай ғана қосамыз:\n",
    "- Чтобы создать входные эмбеддинги, используемые в LLM, мы просто складываем токенные и позиционные эмбеддинги:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45306ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
    "\n",
    "print(input_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79dd3af",
   "metadata": {},
   "source": [
    "- Кіріс деректерді өңдеу процесінің бастапқы кезеңінде кіріс мәтіні жеке токендерге бөлінеді.\n",
    "- Осы сегменттеуден кейін бұл токендер алдын ала анықталған сөздік негізінде токен ID-ларына айналдырылады:\n",
    "\n",
    "<br>\n",
    "\n",
    "- На начальном этапе процесса обработки входных данных входной текст сегментируется на отдельные токены.\n",
    "- После этой сегментации эти токены преобразуются в идентификаторы токенов (ID) на основе предопределённого словаря:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c3683",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/19.webp\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93180d55",
   "metadata": {},
   "source": [
    "# Links\n",
    "\n",
    "* BPE visualizer: https://www.bpe-visualizer.com\n",
    "* Tokenizer visualizer: https://tiktokenizer.vercel.app"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
